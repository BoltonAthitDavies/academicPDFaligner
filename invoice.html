<header id='0' style='font-size:16px'>THE UNIVERSITYof<br>TENNESSEEUR<br>KNOXVILLE</header>
<br><p id='1' data-category='paragraph' style='font-size:18px'>University of Tennessee, Knoxville</p>
<br><p id='2' data-category='paragraph' style='font-size:20px'>TRACE: Tennessee Research and Creative</p>
<br><h1 id='3' style='font-size:22px'>Exchange</h1>
<p id='4' data-category='paragraph' style='font-size:14px'>Doctoral Dissertations</p>
<br><p id='5' data-category='paragraph' style='font-size:16px'>Graduate School</p>
<p id='6' data-category='paragraph' style='font-size:16px'>12-2019</p>
<h1 id='7' style='font-size:20px'>Improving MPI Threading Support for Current Hardware<br>Architectures</h1>
<p id='8' data-category='paragraph' style='font-size:16px'>Thananon Patinyasakdikul<br>University of Tennessee, tpatinya@vols.utk.edu</p>
<p id='9' data-category='paragraph' style='font-size:14px'>Follow this and additional works at: https://trace.tennessee.edu/uk_graddiss</p>
<p id='10' data-category='paragraph' style='font-size:16px'>Recommended Citation</p>
<br><p id='11' data-category='paragraph' style='font-size:14px'>Patinyasakdikul, Thananon, "Improving MPI Threading Support for Current Hardware Architectures. " PhD<br>diss., University of Tennessee, 2019.<br>https://race.tennessee.edu/unk_graddiss/5631</p>
<p id='12' data-category='paragraph' style='font-size:14px'>This Dissertation is brought to you for free and open access by the Graduate School at TRACE: Tennessee<br>Research and Creative Exchange. It has been accepted for inclusion in Doctoral Dissertations by an authorized<br>administrator of TRACE: Tennessee Research and Creative Exchange. For more information, please contact<br>trace@utk.edu.</p>
<p id='13' data-category='paragraph' style='font-size:16px'>To the Graduate Council:</p>
<p id='14' data-category='paragraph' style='font-size:16px'>I am submitting herewith a dissertation written by Thananon Patinyasakdikul entitled "Improving<br>MPI Threading Support for Current Hardware Architectures." I have examined the final electronic<br>copy of this dissertation for form and content and recommend that it be accepted in partial<br>fulfillment of the requirements for the degree of Doctor of Philosophy, with a major in Computer<br>Science.</p>
<p id='15' data-category='paragraph' style='font-size:20px'>Jack Dongarra, Major Professor</p>
<br><p id='16' data-category='paragraph' style='font-size:16px'>We have read this dissertation and recommend its acceptance:</p>
<p id='17' data-category='paragraph' style='font-size:20px'>Michael Berry, Michela Taufer, Yingkui Li</p>
<p id='18' data-category='paragraph' style='font-size:16px'>Accepted for the Council:</p>
<p id='19' data-category='paragraph' style='font-size:14px'>Dixie L. Thompson</p>
<p id='20' data-category='paragraph' style='font-size:14px'>Vice Provost and Dean of the Graduate School</p>
<p id='21' data-category='paragraph' style='font-size:16px'>(Original signatures are on file with official student records.)</p>
<h1 id='22' style='font-size:20px'>Improving MPI Threading Support<br>for Current Hardware Architectures</h1>
<p id='23' data-category='paragraph' style='font-size:16px'>A Dissertation Presented for the<br>Doctor of Philosophy<br>Degree<br>The University of Tennessee, Knoxville</p>
<p id='24' data-category='paragraph' style='font-size:14px'>Thananon Patinyasakdikul<br>December 2019</p>
<p id='25' data-category='paragraph' style='font-size:18px'>ⓒ by Thananon Patinyasakdikul, 2019<br>All Rights Reserved.</p>
<footer id='26' style='font-size:14px'>ii</footer>
<p id='27' data-category='paragraph' style='font-size:18px'>To my parents Thanawij and Issaree Patinyasakdikul, my little brother Thanarat<br>Patinyasakdikul for their love, trust and support.</p>
<footer id='28' style='font-size:14px'>iii</footer>
<h1 id='29' style='font-size:20px'>Acknowledgments</h1>
<p id='30' data-category='paragraph' style='font-size:14px'>I would like to express my deepest gratitude to Dr. Jack Dongarra for giving me the<br>opportunity to become a graduate research assistant at Innovative Computing Laboratory<br>(ICL) and start my journey into the world of high-performance computing (HPC). It is a<br>privilege to work with him and have him as my advisor. His wisdom and experiences will<br>always be the guidance for my future work and life.</p>
<p id='31' data-category='paragraph' style='font-size:16px'>Dr. George Bosilca, my project leader, is an outstanding teacher. I would like to thank<br>him for taking his time to explain and teach me everything I know about MPI. I will always<br>be impressed by his thorough understanding of many projects, and the ability to switch the<br>topic, jumping into a different project on the deepest level on the fly. Outside of work, Dr.<br>Bosilca is very friendly, funny and approachable. I always enjoy working, hanging out, and<br>arguing with him.</p>
<br><p id='32' data-category='paragraph' style='font-size:14px'>I am grateful to my committee, Dr. Michael Berry, Dr. Michela Taufer, and Dr. Yingkui<br>Li for agreeing to serve on my dissertation committee. I greatly appreciate their time,<br>dedication, and invaluable guidance on this dissertation.</p>
<br><p id='33' data-category='paragraph' style='font-size:14px'>I would like to thank my parents for supporting my decision to leave the homeland to<br>pursue my Ph.D. in the United States and providing me with emotional support, love, and<br>understanding. I thank my little brother for taking his time to video call me on the weekends<br>to give me the update on his exciting teenager life.</p>
<br><p id='34' data-category='paragraph' style='font-size:14px'>Last, but definitely not least, I would like to extend my great appreciation toward my<br>colleagues at ICL, from staff scientists to fellow students. Dr. Aurelien Bouteiller, Dr.<br>Thomas Herault, Dr. Damien Genet, Dr. Anthony Danalis, Reazul Hoque, Xi Lou, David<br>Eberius and Yu Pei, and more, for having me in their company, and providing me with great</p>
<footer id='35' style='font-size:14px'>iv</footer>
<p id='36' data-category='paragraph' style='font-size:18px'>(and not SO great) ideas in both work and recreational environment. I wish them all the<br>best.</p>
<footer id='37' style='font-size:14px'>v</footer>
<h1 id='38' style='font-size:20px'>Abstract</h1>
<p id='39' data-category='paragraph' style='font-size:16px'>The Message Passing Interface (MPI) has been the most popular programming paradigm in<br>the high-performance computing (HPC) landscape. The MPI standard provides an efficient<br>communication API with the capability to handle different types of data movements across<br>a variety of network hardware and platforms.</p>
<p id='40' data-category='paragraph' style='font-size:16px'>Since the inception of the MPI standard, the trend in hardware has evolved; a higher<br>number of CPU cores per node introduces more opportunity for thread-parallelism. Dealing<br>with changes in the hardware landscape, threading support has been added to the MPI<br>standard in a later version, with the goal of allowing the user to exploit thread parallelism in<br>MPI applications. Without the need of explicit communication between threads within the<br>same process, multi-threaded MPI is the approach that can relieve stress on the intra-node<br>communication, allowing MPI to focus on only inter-node communication. Nonetheless, this<br>approach comes with its own set of challenges and limitations, which are addressed in this<br>work.</p>
<p id='41' data-category='paragraph' style='font-size:16px'>Threading support for MPI has been defined in the MPI standard since 2008. While many<br>standard-compliance MPI implementations fully support multithreading, they still cannot<br>provide the same level of performance as their non-threading counterpart. This leads to a<br>low adoption rate from applications, and eventually, lesser interest in optimizing threading<br>support for MPI.</p>
<p id='42' data-category='paragraph' style='font-size:18px'>In this work, I propose, implement, and analyze threading optimization of MPI by<br>exploring different tools and approaches to leverage the power of thread parallelism. First, I<br>showed that my multi-threaded MPI benchmark enables MPI developers to stress test their<br>implementation and optimization designs. Second, this work addresses the interoperability<br>between MPI implementations and threading frameworks by introducing a design that gives</p>
<footer id='43' style='font-size:14px'>V1</footer>
<p id='44' data-category='paragraph' style='font-size:18px'>the MPI implementation more control over user-level thread, creating more opportunity for<br>thread utilization in MPI. This design shows up to 7x performance gain in comparison to<br>the original implementation. In the final phase of this study, I propose, implement, and<br>analyze several strategies to address the discovered bottlenecks in the MPI implementation.<br>This novel threading optimization can achieve up to 22x the performance compared to the<br>legacy MPI design in two-sided communication and over 200x in one-sided communication.</p>
<footer id='45' style='font-size:14px'>Vll</footer>
<h1 id='46' style='font-size:22px'>Table of Contents</h1>
<p id='47' data-category='paragraph' style='font-size:18px'>1</p>
<p id='48' data-category='paragraph' style='font-size:20px'>2</p>
<p id='49' data-category='paragraph' style='font-size:18px'>3</p>
<br><p id='50' data-category='paragraph' style='font-size:14px'>Introduction 1<br>1 1 Dissertation Statement . · · . . · · · · · · . · · · 1 . · · · · . + · . · · 3<br>1. 2 Contributions . * * · . · . · · · · . · · · · · · · . · . · * · · . · 4<br>1.3 Dissertation Organization · · . . * . · . . . . * 1  - · · . · . · · · · . 6<br>Background and Literature Review of Related Works 8<br>2.1 Overview · * .  * · · · . · · . · · 8<br>2.2 The MPI Standard · . . . . · . · · · · · . · · · · . · · . · . · · · * . · · · 8<br>2.2.1 Point-to-Point Communication · · - . · 9<br>2.2.2 One-Sided Communication · . · . - * - · · 1 11<br>2.2.3 Threading Support · . · · · · . · . . · · . . 1 13<br>2.3 The Open MPI library · · . · . · * · · 1 * · · · * · 14<br>2.3.1 Modular Component Design * . · . . . · . . · 15<br>2.3.2 Progress Engine . · * 1 . · . . 16<br>2.3.3 Matching Process . · · · · * · · · . 18<br>2.4 Literature Reviews . . · · . · . · · . . . · · · . · . · · . · . · . · · . . . 22<br>Measuring MPI Performance 29<br>3.1 Overview · · · · · · 1 · · · 1 * · · * * · · 1 · · * * · · · * · . 29<br>3.2 Introduction . . . · * · . . · * · . · * · · · . * · · * 1 . . * * . . · · · · . 29<br>3.3 Background · · · · · . · + . * . . * ·  . · · . · · . · · 31<br>3.3.1 Metrics . · · · * . . * . · · · . · - 1 · · · · . · · · · . · 31<br>3.3.2 Workloads . · . · . · · · . . · · 1 · · · · · . . · · · . · . * · . . * 32</p>
<footer id='51' style='font-size:16px'>viii</footer>
<p id='52' data-category='paragraph' style='font-size:18px'>4</p>
<p id='53' data-category='paragraph' style='font-size:20px'>5</p>
<br><p id='54' data-category='paragraph' style='font-size:14px'>3.3.3 Communication Patterns · . · · · · . · . · · . · · · · · · · · · * 33<br>3.3.4 Threading in MPI . · · . · . - - 1 - · · · . · 34<br>3.4 Existing Benchmarks · · · · · · 35<br>3.5 Multirate Benchmark . . * · · · 36<br>3.5.1 Communication Patterns · · · · · · . · · · · · 36<br>3.5.2 Communication Entities · · · · · · . · 37<br>3.5.3 Communicator's Effect · . . . · . * . . . 38<br>3.6 Experimental Evaluation . · · · * · · . 39<br>3.6.1 Communication Patterns . · · * · · · 40<br>3.6.2 Variable workload + . · . . · + · · · · · 50<br>3.6.3 Multithread MPI . · · · · · · · . 52<br>3.7 Conclusion 1 * . . + · . . · · · . · · · · . · · · . · 57<br>Advance Thread Synchronization 58<br>4.1 Overview - · · · . · · . · * - - · · · · · · * . · · · * · . · . · . · · . · . 58<br>4.2 Introduction · . * . . . · . · · * · . * . * * · * . . · + . · . · 58<br>4.3 Progress Engine Serialization . · . · * · - 1 · · . · 60<br>4.4 Synchronization Object · · · . . * . · · . · . 61<br>4.5 Experimental Evaluation · · 1 · · . * 66<br>4.6 Ongoing Research a · * . . . 67<br>4.6.1 User-Level Extension  · · * · · · . · 67<br>4.6.2 Thread Pool * · . . 74<br>4.6.3 Multi-Threaded Progress Engine * . · * . - · · . 76<br>4.7 Conclusion * 1 . + · · · · · · · · · · · · 76<br>Design of True Thread Concurrency in MPI 77<br>5.1 Overview . . * - * · · · . · · · * · · 77<br>5.2 Background · . · · · · * · · · · , · · 78<br>5.3 Design and Implementation · · . · a -  . · * · * . 82<br>5.3.1 Communication Resources Instance · · · . * * . * * · . * · · · · 82<br>5.3.2 Try-lock Semantics 1 · · · · . · * · · . · · · 83<br>·</p>
<footer id='55' style='font-size:16px'>1X</footer>
<p id='56' data-category='paragraph' style='font-size:14px'>5.3.3 Concurrent Sends . . · · · · . · · · · · · · · · . · · · · . · · · · ■ 84<br>5.3.4 Concurrent Progress . · . · · * · . . - · · * + - · * · · · 86<br>5.3.5 Concurrent Matching · · . . · · 86<br>5.4 Experimental Evaluation · · · · . * · . · 88<br>5.4.1 Concurrent Sends · · . · · · * ★ . . . 1 · · · · 90<br>5.4.2 Concurrent Progress · · . · · . · 91<br>5.4.3 Concurrent Matching · · . . . * . . . 92<br>5.4.4 Message Overtaking · . * · · · 93<br>5.4.5 Current State of MPI Threading · · · · . · 94<br>5.4.6 RMA Performance . · · · . · · . * · · · · . 95<br>5.5 Optimization Suggestions · . · · . · * · · . · · 97<br>5.6 Conclusion * · . · . · . · · · . · · · . · . · · . · · . . · 99<br>6 Conclusion and Future Work 101<br>6.1 Conclusion · · · · · · · . · . · . · · . · · · * . · · · · . · · + · . · · · 101<br>6.2 Future Work * . · . . · . · . · · · · . · · · · . . · 104<br>. · . · · . · . · · . · .<br>Bibliography 106<br>Appendices 116<br>A Multirate Benchmark User Guide · · . · · · · · · · · · * 1 . 117<br>B Thread Synchronization Object MPI Extension · . . · . · · . · . · . · . · 119<br>C MPI One-sided Window Operations · · · · · . · · · . · · · · . · 124<br>Vita 126</p>
<footer id='57' style='font-size:18px'>x</footer>
<h1 id='58' style='font-size:20px'>List of Tables</h1>
<p id='59' data-category='paragraph' style='font-size:14px'>4.1 PaRSEC performance speedup from MPIX_Sync API. · · · . · · · · . · . 73<br>5 1 Configuration of the testing systems, Alembert and Trinitite. . . . · . . · . · 89<br>5 2 Software Performance Counters information from last data point of the<br>experiment · . . . . · . . · · . . . · · · · . · · · . . · · . · . . . . · . . · . , 91</p>
<footer id='60' style='font-size:16px'>X1</footer>
<h1 id='61' style='font-size:20px'>List of Figures</h1>
<p id='62' data-category='paragraph' style='font-size:14px'>2.1 Example of Open MPI framework layers. . . · . · · . · . . . · . · · · · 15<br>2.2 Interaction between BTL framework and pml/ob1 in Open MPI. · · · . · · · 17<br>2.3 Every component registers their progress routine to the progress engine. · . · 19<br>2.4 Matching Process implementation · · . . . · - · · · . · · · · · · 19<br>2.5 Matching process implementation with ordering guarantees · · · . · · . . . · 21<br>3.1 The use of multiple threads to increase communication throughput. · · · . · 33<br>3.2 One-to-many and many-to-one communication pattern . · . . · . · . . . · · 34<br>3.3 CPU core mapping to MPI ranks for pairwise pattern. · · · . · · · · . · · 37<br>3.4 All-to-all communication can be used to make sub-patterns, such as many-to-<br>many (a), many-to-one (b), and one-to-many (c). · . · . . . . · · · . . · . · 38<br>3.5 Pairwise message rate for a message size of 1,024 bytes, w = 128. · · · · · · 42<br>3.6 Pairwise message rate for a message size of 4,096 bytes, w = 128. · · . · · . 42<br>3.7 Many-to-one message rate with a message size of 1,028 bytes, W = 128. · · · 44<br>3.8 Many-to-one message rate with a message size of 4,096 bytes, W = 128. · · · 45<br>3.9 The one-to-many message rate with a message size of 1,024 bytes, W = 128. 46<br>3.10 The one-to-many message rate with a message size of 4,096 bytes, W = 128. 46<br>3.11 The many-to-many communication performance with a fixed number of<br>receivers. . · . . . · . . · · · . . · · . · . . · . . . . · . · · . . . · . . . . · 49<br>3.12 The many-to-many communication performance with a fixed number of senders. 49<br>3.13 Message rate (1,024 bytes) on different communication patterns on multiple<br>window sizes. . · · . . . · · · . · · · · · . · · . · . · · . · · · · · · . . · · 51</p>
<footer id='63' style='font-size:16px'>X11</footer>
<p id='64' data-category='paragraph' style='font-size:14px'>3.14 Performance difference between two Open MPI release versions; 1,024 bytes<br>pairwise, process to process mode. · · . · . · · . · . · · . · · . · . · · . · . 53<br>3.15 Minimum cost of thread safety. · . · . · . · · · · · . · . · · · · · · · . · 53<br>3.16 Zoomed-in graph for pairwise message rate for thread mode with btl/uct to<br>demonstrate the effect of the communicator. · · · · · · · · · . · . · · . · · · 56<br>4.1 Cost of lock acquiring on Intel Xeon E5-2650 v3 (Haswell) · . · . · · . · · · 61<br>4.2 MPI_Wait* operation implementation in multi-threaded scenario. · · · · 64<br>4.3 Performance gain from utilizing thread synchronization object in MPI_Wait<br>implementation. . · · · · . · · · . · . · · . · . · . · · . 67<br>4.4 Message Rate in thread over-subscription scenario. · . · · · · · · · · · · · · · 68<br>4.5 The MPIX_Sync API design. . . . . · . · · · . · · · · · · , 70<br>4.6 MPIX_Sync_query performance comparing to MPI_ Testsome. · · · · · · · . 72<br>4.7 Thread pool design utilizing the synchronization object. · · · · . . · · . . · · 75<br>4.8 MPLPack performance when utilizing threads in the threadpool design. · . · 75<br>5.1 Matching process with serial and concurrent progress engine. . · · · . . · . 88<br>5.2 Zero byte message rate on different strategies. · · . · · · . · · · . . · · · 91<br>5.3 Zero byte message rate when the message ordering is not enforced · · · · · 94<br>5.4 Zero byte message rate from different state-of-the-art MPI implementations. 96<br>5.5 Multi-threaded One-sided communication performance. . . · · . · · 98</p>
<footer id='65' style='font-size:18px'>xiii</footer>
<h1 id='66' style='font-size:22px'>Chapter 1</h1>
<h1 id='67' style='font-size:20px'>Introduction</h1>
<p id='68' data-category='paragraph' style='font-size:18px'>The Message Passing Interface (MPI) is nearly ubiquitous in high-performance computing<br>(HPC) according to Bernholdt et al. more than 90% of Exascale Computing Project (ECP)<br>and Advanced Technology Development and Mitigation (ATDM) application proposals use<br>it either directly or indirectly. Therefore, the availability of high-quality, high-performance,<br>and highly scalable MPI implementations which address the needs of applications and the<br>challenges of novel hardware architectures is fundamental for the performance and scalability<br>of parallel applications.</p>
<p id='69' data-category='paragraph' style='font-size:16px'>The MPI standard provides an efficient and portable communication-centric API that<br>defines a variety of capabilities to handle different types of data movement across processes,<br>such as point-to-point messaging, collective communication, one-sided remote memory<br>access (RMA), and file support (MPI-IO) Forum (2015). This ensemble of communication<br>capabilities gives applications a toolbox for satisfying complex and irregular communication<br>needs in a setup that maintains portability and performance across different hardware<br>architectures and operating systems. Owing to these characteristics, many scientific<br>applications have adopted MPI as their communication infrastructure and, therefore, rely<br>on the efficiency of the MPI implementation to deliver the best communication performance<br>for their applications across different networking hardware on various platforms.</p>
<p id='70' data-category='paragraph' style='font-size:16px'>Recent hardware developments, with higher numbers of cores per chip, even with<br>higher frequency, have shifted the balance of computation VS. communication in favor<br>of computations, which have become faster and more energy efficient. Over the last</p>
<footer id='71' style='font-size:14px'>1</footer>
<p id='72' data-category='paragraph' style='font-size:20px'>decade alone, theoretical node-level compute power has increased 19x, while bandwidth<br>available to applications has seen an increase by a factor of 3x only, resulting in a net<br>decrease in bytes per floating-point operation (FLOP) by 6x Rumley et al. (2017). An<br>increased rate of computation needs to be sustained by a matching increase in memory<br>bandwidth, but physical constraints- -such as the conductivity and the thermal capacity of<br>the network cables' materials set hard limits on the latency and bandwidth of data transfers.<br>The current solution to overcome these limitations has increased the number of memory<br>hierarchies, with orders of magnitude variation in cost and performance between them.<br>Essentially, current architectures represent execution environments where data movement<br>is the most performance and energy critical component. This shift has greatly impacted the<br>traditional programming approach where each computational core corresponds to a unique<br>process and every data movement passes through a message-passing layer. As the intra-node<br>and inter-process communication costs started to rise, efforts began to move applications<br>toward a more dynamic and / or flexible programming paradigm.</p>
<br><p id='73' data-category='paragraph' style='font-size:16px'>Using the combination of processes and threads becomes one of the promising solutions,<br>as the approach is capable of relieving the pressure on the memory infrastructure. One of the<br>advantages of this approach comes from the benefit that no explicit communication between<br>threads in the same process is necessary. Although the use of multiple threads to alleviate<br>the pressure on intra-node data movement seems like an intuitive approach, it generates an<br>entire set of new challenges in both programmability and communication levels.</p>
<p id='74' data-category='paragraph' style='font-size:20px'>Firstly, the challenge is to guarantee for the thread safety of the applications. Multiple<br>threads are likely to create 'race conditions' if they try to access or update the same<br>memory, affecting the correctness or even corrupting the state of the application if<br>not handled appropriately. In order to provide thread safety and prevent potential<br>race conditions in applications, several thread synchronization mechanisms are available.<br>However, thread synchronization often involves the interlocked memory operations or a<br>kernel-level transaction, translated into an extra overhead for the multi-threaded application.<br>Moreover, other than in the application level, the MPI implementations, as the message<br>passing layer, is also susceptible to the race-conditions from threads and has to be designed<br>in a way that provides thread safety with minimal overhead in multi-threaded environments.</p>
<footer id='75' style='font-size:14px'>2</footer>
<p id='76' data-category='paragraph' style='font-size:14px'>Secondly, the communication level challenge stems from the nature of the non-<br>deterministic behavior of the threads. Generally, threads behave better when they are more<br>independent or loosely coupled, but more flexibility translates into reduced ordering between<br>actions in different threads -and unfortunately this also includes the communication. The<br>out-of-order communication from threads can become a significant problem for the MPI<br>implementations. It becomes a chronic symptom of the lack of send determinism in<br>applications Guermouche et al. (2011). That being said, in a communication paradigm<br>other than the MPI, this could have been a minor issue (as an example in an Active<br>Message Eicken et al. (1992) context). The MPI standard mandates a first-in, first-out<br>(FIFO) message ordering guarantee for simplicity of programming, relieving the user of the<br>burden of maintaining their own messages order. Unfortunately, the MPI standard does<br>not take thread non-deterministic behavior into consideration, as support for multi-threaded<br>environment has been added in the later version. This poses more challenges to optimize<br>the multi-threaded support of MPI. Furthermore, the lack of the interoperability between<br>the threading frameworks and the MPI contributes to more optimization limitations for the<br>MPI developers, as the MPI implementations do not have necessary thread information to<br>evaluate and make optimization decisions accordingly.</p>
<br><p id='77' data-category='paragraph' style='font-size:14px'>Current state-of-the-art MPI implementations are struggling to support a large number<br>of concurrent communications and are under-utilizing thread parallelism in multi-threaded<br>environments, resulting in suboptimal performance in the communication. With that in<br>mind, this study proposes several strategies to enhance MPI communication performance<br>in multi-threaded environments through an increased concurrency on different levels of the<br>MPI implementation for both one-sided and two-sided MPI communications.</p>
<p id='78' data-category='paragraph' style='font-size:18px'>1.1 Dissertation Statement</p>
<p id='79' data-category='paragraph' style='font-size:14px'>While most current state-of-the-art MPI implementations fully support threading environ-<br>ments in MPI, the performance of the existing threading environment is still far behind its<br>non-threading counterparts. This disparity, in turn, creates the 'chicken-and-egg' problem<br>a low adoption rate for threading environments by the MPI users leads to a low interest in</p>
<footer id='80' style='font-size:14px'>3</footer>
<p id='81' data-category='paragraph' style='font-size:16px'>threading optimization from the community of MPI developers. During the length of this<br>study, I also found that there were limited choices of performance evaluation tools for MPI.<br>While the existing tools adequately measure basic communication performance, they fail to<br>capture several aspects of the MPI communication in the multi-threaded environment and are<br>therefore unable to expose its shortcomings. This made the task of optimizing MPI threading<br>performance even more challenging. Furthermore, the lack of interoperability between<br>the threading frameworks and the MPI implementation adds to the existing challenges in<br>performance optimization for threading in MPI.</p>
<p id='82' data-category='paragraph' style='font-size:16px'>Therefore, in this study, I work to optimize the multi-threaded environment from an<br>MPI implementation standpoint. This is done by starting with a thorough investigation of<br>prior studies on MPI, finding areas where existing MPI implementation cannot perform up<br>to a satisfactory mark in a multi-threaded environment and thus require improvement, and,<br>finally, design and implementation of solutions that enhance the performance of MPI in a<br>multi-threaded environment.</p>
<h1 id='83' style='font-size:20px'>1.2 Contributions</h1>
<p id='84' data-category='paragraph' style='font-size:16px'>In this study, I contribute to the optimization of multi-threaded MPI in two broad ways:<br>(1) By introducing a new tool for evaluating performance of an MPI implementation, a<br>tool which addresses some of the shortcomings of the existing benchmarks and (2) by<br>proposing, implementing, and analyzing a set of threading performance optimizations in<br>a particular MPI implementation, Open MPI. The latter covers a number of novel and non-<br>trivial strategies that highlight portable ways to fully utilize thread parallelism in an MPI<br>implementation.</p>
<p id='85' data-category='paragraph' style='font-size:18px'>Enhancement of Performance Evaluation Tools for MPI</p>
<p id='86' data-category='paragraph' style='font-size:16px'>I address the lack of flexibility in performance evaluation tools for multi-threaded MPI<br>and increasing the number of powerful toolkits with various capabilities of performance<br>assessment. This is done by, first, investigating the currently available performance<br>measurement tools for multi-threaded MPI, to assess their strengths and weaknesses. A</p>
<footer id='87' style='font-size:14px'>4</footer>
<p id='88' data-category='paragraph' style='font-size:14px'>major challenge of using the existing benchmarks is that they offer very limited capabilities<br>for example, the lack of ability to adjust the workload or communication pattern. In order<br>to better evaluate the multi-threaded performance, these gaps need to be overcome. This<br>research proposes a solution-the Multirate benchmark Patinyasakdikul et al. (2019), which<br>allows users to evaluate benchmarks on various aspects. This highly flexible benchmark<br>exposes the shortcomings of a multi-threaded implementation of the MPI through multiple<br>communication patterns and flexible workload, allowing MPI developers to quickly compare<br>performance in a threading environment to a non-threading environment. The potential<br>of the proposed benchmark is demonstrated by evaluating the current state-of-the-art MPI<br>implementations such as Intel MPI, Open MPI and MPICH. I strongly believe that this<br>will enable the current and future MPI developers to more efficiently optimize their MPI<br>implementation.</p>
<p id='89' data-category='paragraph' style='font-size:18px'>Strategies to Optimize Threading Performance in MPI</p>
<p id='90' data-category='paragraph' style='font-size:14px'>I propose several strategies to optimize the threading performance by addressing the lack<br>of interoperability between the MPI implementation and the threading frameworks. I also<br>propose a novel design with different approaches to harness the power of thread parallelism<br>for the MPI implementation.</p>
<p id='91' data-category='paragraph' style='font-size:14px'>· Better Thread Synchronization: Current thread synchronization approaches<br>in modern MPI implementations are highly inefficient, resulting in threads being<br>inadequately organized. This creates unnecessary contention in the critical MPI<br>components, which results in time wasted causing degradation in overall performance.<br>To equip the MPI implementation with more control over user-level threads and<br>give the MPI implementations more opportunity to optimize for threading support,<br>I introduce the concept of the thread synchronization object. The benefits of my<br>design is demonstrated by employing the synchronization object to commandeer the<br>access to the MPI progress engine, thereby reducing the unnecessary lock contention<br>from the original approach. I also demonstrate other potential use cases of the thread<br>synchronization object in real-world applications.</p>
<footer id='92' style='font-size:14px'>5</footer>
<p id='93' data-category='list' style='font-size:16px'>· Better Resource Management: I propose and implement a design of Communica-<br>tion Resource Instances (CRIs) Patinyasakdikul et al. (2019) for an efficient allocation<br>of resources. I studied the impact of resource contention in multi-threaded MPI in the<br>current design which led to the discovery of a number of shortcomings. This acted<br>as an inspiration to come up with solutions that can fill the voids that the legacy<br>resource allocation strategies have left. My implemented design provides MPI with a<br>simpler design to allocate more resources for threads and help alleviating the resource<br>contention in the MPI implementation. I also propose multiple strategies to incorporate<br>CRI into MPI core functionality to extract more performance from threads. I discuss<br>its impact on both one-sided and two-sided communication in multi-threaded MPI.</p>
<p id='94' data-category='paragraph' style='font-size:16px'>· Optimization Suggestions: I summarize my study and propose a compiled list<br>of suggestions to both the MPI developers and the MPI users to fully harness the<br>power of threading in MPI. The optimization presented in this dissertation has been<br>incorporated into the Open MPI development branch. 1 It is to be noted that from a<br>software perspective, all my optimization work has been accepted by the Open MPI<br>community and released publicly with Open MPI 4.0.</p>
<h1 id='95' style='font-size:20px'>1.3 Dissertation Organization</h1>
<p id='96' data-category='paragraph' style='font-size:14px'>The rest of this dissertation is organized as follows:</p>
<p id='97' data-category='list' style='font-size:16px'>· Chapter 2 provides the high-level details from sections of the MPI standard which are<br>related to the focus of this study. I discuss the trends from the prior studies around<br>the topic of the use and the optimization of the threading environment in the HPC<br>community, and ultimately, in the MPI communication.<br>· Chapter 3 discusses the the motivation for my proposed multi-threaded MPI bench-<br>mark along with its design, and evaluate its capabilities by performing measurements<br>on the current state-of-the-art MPI implementations, assess their strengths and<br>weaknesses, in both threading and non-threading environments.</p>
<br><p id='98' data-category='footnote' style='font-size:18px'>1https: / /github.com/open-mpi/ ompi</p>
<footer id='99' style='font-size:16px'>6</footer>
<p id='100' data-category='list' style='font-size:18px'>Chapter 4 presents my design and implementation of the thread synchronization object,<br>and illustrates how the design can provide the MPI implementation with more control<br>and better utilization over the user-level threads, and the ability to redirect them. I<br>demonstrate the potential of the thread synchronization object design, evaluate its<br>performance and discuss ongoing research collaborations originated from this design.<br>· Chapter 5 presents my solution for better resource allocations in threading environ-<br>ments through the design of the CRIs, along with the assignment strategies for better<br>thread concurrency in different levels of the MPI implementations. I evaluate the<br>CRI implementation in both one-sided and two-sided communications. I discuss my<br>findings and provide additional suggestions for optimizing the MPI in multithreaded<br>environments.<br>· Chapter 6 concludes this dissertation with the summary of my findings, with<br>suggestions for the multi-threaded support in the MPI, and I discuss my future<br>directions.</p>
<footer id='101' style='font-size:14px'>7</footer>
<h1 id='102' style='font-size:22px'>Chapter 2</h1>
<h1 id='103' style='font-size:22px'>Background and Literature Review of<br>Related Works</h1>
<h1 id='104' style='font-size:20px'>2.1 Overview</h1>
<p id='105' data-category='paragraph' style='font-size:18px'>This chapter presents the high-level background knowledge related to the topics of this<br>study. It focuses mainly on the basic point-to-point communication, both one-sided and<br>two-sided. I discuss the MPI standard, especially the threading support of MPI, along with<br>the high-level design of Open MPI, an open-source MPI implementation used as the base<br>MPI implementation for this study, and finally, present the prior studies of several aspects,<br>challenges and proposed solutions for optimizing multithreaded performance in MPI.</p>
<h1 id='106' style='font-size:20px'>2.2 The MPI Standard</h1>
<p id='107' data-category='paragraph' style='font-size:16px'>The first MPI standard Forum (2015) was published by the MPI forum in 1994 as a<br>revolutionary programming paradigm for high-performance computing. The MPI forum<br>is continuously maintaining and releasing the new specifications and adds more functionality<br>to the MPI API to the present day. The current version of MPI standard as of this study is<br>MPI standard 3.1, published in June of 2015. The threading support in the MPI standard<br>was originally not well defined. The official threading support from the MPI standard begins<br>from the standard version 2.1 in September, 2008. In this section, I discuss the MPI standard</p>
<footer id='108' style='font-size:14px'>8</footer>
<p id='109' data-category='paragraph' style='font-size:16px'>API of interest for the scope of this study, from point-to-point communication to the newly<br>added one-sided communication, and the multithread environment supports from the MPI.</p>
<h1 id='110' style='font-size:20px'>2.2.1 Point-to-Point Communication</h1>
<p id='111' data-category='paragraph' style='font-size:14px'>The point-to-point communications are the communication between two MPI processes.<br>The operation involves sender and a receiver, always in matching pairs. It is the most<br>basic form of communication defined with the original MPI standard from 1994. Other<br>than the user-level API, point-to-point communication also serves as the bedrock to a more<br>sophisticated communication provided with the MPI standard, such as collective operations.<br>It is important to optimize the performance of point-to-point communication as it may<br>translate into better performance overall for the MPI implementation.</p>
<p id='112' data-category='paragraph' style='font-size:14px'>The MPI standard provides multiple flavors of the API for point-to-point communi-<br>cations, allowing the user to be more specific on the behavior of the communication and<br>optimize the application to their needs. However, in this study, I only discuss the most basic<br>point-to-point communication API.</p>
<h1 id='113' style='font-size:16px'>Send and Receive API</h1>
<p id='114' data-category='paragraph' style='font-size:16px'>int MPI_Send (const void *buf, int count, MPI_Datatype datatype,<br>int dest, int tag, MPI_Comm comm)</p>
<p id='115' data-category='paragraph' style='font-size:14px'>int MPI_Isend(const void *buf , int count, MPI_Datatype datatype ,<br>int dest, int tag, MPI_Comm comm, MPI_Request *request)<br>int MPI_Recv (void *buf , int count, MPI_Datatype datatype,<br>int source, int tag, MPI_Comm comm, MPI_Status *status)</p>
<p id='116' data-category='paragraph' style='font-size:14px'>int MPI_Irecv (void *buf , int count, MPI_Datatype datatype,<br>int source, int tag, MPI_Comm comm , MPI_Request *request)</p>
<p id='117' data-category='paragraph' style='font-size:14px'>The MPI standard provides message matching by tag and also guarantees that every<br>message will be received in a FIFO order. In short, the first send will always get matched</p>
<footer id='118' style='font-size:14px'>9</footer>
<p id='119' data-category='paragraph' style='font-size:16px'>with the first receive of the same tag, relieving the user of the burden of tracking message<br>sequences.</p>
<br><p id='120' data-category='paragraph' style='font-size:16px'>The API comes in two modes: synchronous and asynchronous (asynchronous API has a<br>prefix of 'I'). Some might refer to synchronous as blocking, and asynchronous as non-blocking,<br>due to its behavior. As the name suggests, the synchronous call waits until the operation<br>is completed, at least locally, before returning from the call, while asynchronous only issues<br>the intent for communication and return to the user immediately. The asynchronous API<br>gives the user a "request," an opaque handle associated with the operation for the user to<br>check for its completion later with MPI_ W ait or MPI_Test variants. The asynchronous API<br>allows for more flexibility of the application as the user can ask for the message completion<br>only when it is needed, and avoid the implicit synchronization that usually comes with<br>the synchronous (blocking) communication while also provide the possibility of the overlap<br>between computation and communication.</p>
<h1 id='121' style='font-size:16px'>Wait and Test API</h1>
<p id='122' data-category='paragraph' style='font-size:16px'>int MPI_Wait (MPI_Request *request, MPI_Status *status)</p>
<p id='123' data-category='paragraph' style='font-size:14px'>int MPI_Waitsome (int incount, MPI_Request array_of_requests [] ,<br>int *outcount, int array_of_indices [] ,<br>MPI_Status array _of_statuses [] )</p>
<p id='124' data-category='paragraph' style='font-size:20px'>int MPI_Waitany (int count, MPI_Request array_of_requests [] ,<br>int *index, MPI_Status *status)</p>
<p id='125' data-category='paragraph' style='font-size:20px'>int MPI_Waitall (int count, MPI_Request array_of_requests [] ,<br>MPI_Status *array_of_statuses)</p>
<p id='126' data-category='paragraph' style='font-size:20px'>int MPI_Test (MPI_Request *request, int *flag, MPI_Status *status)</p>
<p id='127' data-category='paragraph' style='font-size:16px'>int MPI_Testsome (int incount, MPI_Request array _of_requests [] ,</p>
<footer id='128' style='font-size:16px'>10</footer>
<h1 id='129' style='font-size:14px'>int *outcount, int array_of_indi ces [] ,<br>MPI_Status array_of_statuses [] )</h1>
<p id='130' data-category='paragraph' style='font-size:20px'>int MPI_Testany (int count, MPI_Request array_of_requests [] ,<br>int *index, int *flag, MPI_Status *status)</p>
<p id='131' data-category='paragraph' style='font-size:18px'>int MPI_Testall (int count, MPI_Request array_of_requests [],<br>int *flag, MPI_Status array_of_statuses [] )</p>
<p id='132' data-category='paragraph' style='font-size:16px'>For asynchronous operation, the MPI standard provides two major ways of checking for<br>completion through wait and test API. Similar to the point-to-point API, the wait API is a<br>synchronous routine and will only return when the condition is met (number of completed<br>requests) while test is an asynchronous routine, which will return immediately but provide<br>the means for the user to get the information of the requests. The MPI standard offers 4<br>flavors of the wait / test operation, a single request and multiple requests (some, any, all). As<br>the name suggests, for 'some', the user can test for a subset of requests by providing the<br>desired number with the API. The user can check for the completion of one or more requests<br>through 'any' API and completion of every request through 'all' API.</p>
<h1 id='133' style='font-size:22px'>2.2.2 One-Sided Communication</h1>
<p id='134' data-category='paragraph' style='font-size:16px'>In addition to two-sided communication, the MPI-2.1 standard provides support for one-<br>sided RMA communication. This support allows an MPI implementation to directly expose<br>hardware Remote Direct Memory Access (RDMA), a feature which is present on some high-<br>performance networks, e.g., Infiniband, and Cray Aries. This allows the MPI implementation<br>to offload communication directly to the hardware. In addition, the one-sided model<br>separates communication (data movement) from the synchronization (completion). The<br>standard defined API for one-sided communication are the following.</p>
<p id='135' data-category='paragraph' style='font-size:20px'>Window Initialization / Finalization</p>
<p id='136' data-category='paragraph' style='font-size:18px'>int MPI_Win_ create (void *base, MPI_Aint size, int disp_unit,</p>
<footer id='137' style='font-size:16px'>11</footer>
<h1 id='138' style='font-size:16px'>MPI_Info info, MPI_ Comm comm , MPI_Win *win)</h1>
<br><h1 id='139' style='font-size:18px'>int MPI_Win_free (MPI_Win *win)</h1>
<h1 id='140' style='font-size:16px'>Data Movement</h1>
<p id='141' data-category='paragraph' style='font-size:16px'>int MPI_Put (const void *origin_addr , int origin_count, MPI_Datatype<br>origin_datatype, int target_rank, MPI_Aint target_disp,<br>int target_count, MPI_Datatype target_datatype, MPI_Win win)</p>
<p id='142' data-category='paragraph' style='font-size:16px'>int MPI_Get (void *origin_addr , int origin_count, MPI_Datatype<br>origin_datatype, int target_rank, MPI_Aint target_disp,<br>int target_count, MPI_Datatype target_datatype, MPI_Win win)</p>
<p id='143' data-category='paragraph' style='font-size:18px'>int MPI_Accumulate (const void *origin_addr, int origin_count,<br>MPI_Datatype origin_datatype, int target_rank,<br>MPI_Aint target_disp, int target_count,<br>MPI_Datatype target_datatype, MPI_Op op, MPI_Win win)</p>
<h1 id='144' style='font-size:20px'>Operation Completion (Examples)</h1>
<p id='145' data-category='paragraph' style='font-size:16px'>int MPI_Win_flush (int rank, MPI_Win win)<br>int MPI_Win_flush_all (MPI_Win win)<br>int MPI_Win_lock(int lock_type, int rank, int assert , MPI_Win win)<br>int MPI_Win_lock_all (int assert, MPI_Win win)<br>int MPI_Win_fence (int assert, MPI_Win win)</p>
<p id='146' data-category='paragraph' style='font-size:14px'>The API offers three ways of data movement. MPI_Put and MPI_Get offer remote write<br>and read operation while MPI_Accumulate allows the user to perform atomic mathematical<br>operations such as addition or multiplication on the target buffer. The operations are carried<br>out by the source without involving the target. However, since anyone can read or write to<br>the same target buffer at any time, the user is responsible for keeping track of their data<br>accessing order through the synchronization.</p>
<footer id='147' style='font-size:14px'>12</footer>
<p id='148' data-category='paragraph' style='font-size:16px'>The MPI API provides the means of synchronizing through an abstraction of 'window'.<br>Every MPI process exchanges the necessary information beforehand at the window creation<br>through MPI_Win_create. The example of the window operations are listed above. The entire<br>window operation API is listed in the appendix C. Unlike the two-sided communication, the<br>information of peers and target buffer is already exchanged in this window creation process.<br>Hence, the message matching operation is no longer required after the actual communication.<br>There are multiple flavors of synchronization on the window, but in short, performing any<br>synchronization operation on the window will complete outstanding operations associated<br>with that window. For example, fence completes every outstanding operation on the window<br>for every peer, while lock completes the operation for between the calling process and the<br>target process only.</p>
<h1 id='149' style='font-size:20px'>2.2.3 Threading Support</h1>
<p id='150' data-category='paragraph' style='font-size:18px'>The MPI-3.1 standard Forum (2015) provides four levels of threading support. During MPI<br>initialization, more precisely during MPI.INIT.THREAD, users can marshal with the MPI<br>implementation the desired thread level for the application.</p>
<p id='151' data-category='list' style='font-size:16px'>· MPI_THREAD _SINGLE: The most restrictive setting where a single thread must<br>exist in the application, independent if they make MPI calls or not;</p>
<p id='152' data-category='list' style='font-size:16px'>· MPI_THREAD _FUNNELED: Multiple threads can coexist in the application- but<br>only one, the master thread (i.e., the one that initialized the MPI library), is allowed<br>to performing MPI operations;</p>
<p id='153' data-category='list' style='font-size:16px'>· MPI_THREAD _SERIALIZED: Multiple threads can coexist in the application, and<br>the application is responsible for serializing their MPI calls, in order to guarantee that<br>only a single thread will perform MPI operations at any time;</p>
<p id='154' data-category='list' style='font-size:14px'>· MPI_THREAD MULTIPLE: Multiple threads exist in the application and every<br>thread can perform MPI operations at any time, without restriction on the ordering<br>or serialization.</p>
<footer id='155' style='font-size:16px'>13</footer>
<p id='156' data-category='paragraph' style='font-size:14px'>From a purely pragmatic point of view, most of these thread support levels have little<br>reason to exists nowadays, but it is acknowledged that it might be needed on some esoteric<br>hardware, with extremely restrictive thread support from the operating system. Anyhow,<br>with the current hardware architectures, there is no possibility of race conditions for the<br>SINGLE, FUNNELED, or SERIALIZED mode. Thus, current MPI implementations are<br>not providing any protection for these modes as there is no need to spend the unnecessary<br>cost. Thread safety is therefore only provided if the user initializes MPI with MULTIPLE.<br>This study is focused only on the MPI_THREAD MULTIPLE mode, as it is the only<br>mode that allows for thread concurrency.</p>
<p id='157' data-category='paragraph' style='font-size:14px'>The benefit of multi-thread environments has been explored since its inception with the<br>MPI-2.1 standard. One of the proposed benefits is to decrease the memory footprint of<br>the MPI application. By taking advantage of thread memory space, every thread in the<br>same process can access the same space of memory, reducing the need for multiple copies<br>of the same data. Moreover, utilizing the multi-thread environment reduces the need for<br>intra-node explicit communication as the threads can simply access other threads' memory.<br>Another benefit of threading is to increase the throughput for messages of smaller size. As<br>MPI implementations are highly optimized for sending large messages through sophisticated<br>pipeline algorithms, sending a high volume of smaller messages is still a challenging aspect<br>to optimize for. This study contributes to the efforts to improve small message throughput<br>by utilizing threads to send multiple messages in parallel.</p>
<h1 id='158' style='font-size:18px'>2.3 The Open MPI library</h1>
<p id='159' data-category='paragraph' style='font-size:14px'>While this study is generic and can be applied to any MPI implementation, all designs<br>and engineering aspects were implemented in Open MPI. In this section, I present multiple<br>aspects of interests of Open MPI for this study. The Open MPI Gabriel et al. (2004) is<br>one of the MPI implementations that is fully compliant with the MPI standard 3.1. It<br>is an open source MPI implementation that, like most open-source projects, is developed<br>and maintained by an active community of volunteers, the Open MPI community. This<br>community consists of a large number of volunteers, together with participants from a variety</p>
<footer id='160' style='font-size:14px'>14</footer>
<p id='161' data-category='paragraph' style='font-size:18px'>of organizations from both academia and industry. With its open-source nature, Open MPI<br>has been used as the base of multiple vendor's MPI implementations such as Fujitsu, Bull,<br>and IBM Spectrum MPI, driving some of the most powerful supercomputers in the world at<br>the time of this study, Summit Vazhkudai et al. (2018).</p>
<h1 id='162' style='font-size:22px'>2.3.1 Modular Component Design</h1>
<p id='163' data-category='paragraph' style='font-size:16px'>Open MPI employs a modular design where multiple components can work independently<br>through a well designed, standardized API with frameworks (Figure 2.1). The components<br>can be plugged easily into a framework as long as they provide the necessary API for the<br>framework to operate. The framework-component design allows for multiple implementations<br>of the same functionality. For example, the coll framework provides the functionality<br>of collective operations. Any developer can create a component under coll to plug their<br>implementation of collective operations such as the broadcast or allreduce operation, and<br>have Open MPI invoke their component for the MPI_Broadcast or MPI_Allreduce call.</p>
<p id='164' data-category='paragraph' style='font-size:16px'>Operating in a multi-threaded environment presents a different level of challenges to<br>the Open MPI components, as the individual components serve in a different capacity, and<br>in some cases, are designed to interact with the different set of hardware with different<br>capabilities and limitations. This study mainly focuses on the multi-threaded operation of<br>2 major Open MPI frameworks which provide the basic communication support; the Point<br>Messaging Layer (PML) and the Byte Transporting Layer (BTL).</p>
<figure><img id='165' style='font-size:14px' alt="Open MPI
OPAL OMPI ORTE
BTL PML MTL coll" data-coord="top-left:(274,1149); bottom-right:(1012,1381)" /></figure>
<p id='166' data-category='paragraph' style='font-size:20px'>Figure 2.1: Example of Open MPI framework layers.</p>
<footer id='167' style='font-size:18px'>15</footer>
<h1 id='168' style='font-size:18px'>Point Messaging Layer (PML)</h1>
<p id='169' data-category='paragraph' style='font-size:14px'>PML is a framework for point-to-point data movement. The framework provides functions<br>such as send, receive, and message matching for the Open MPI. Currently, there are multiple<br>components in the PML framework such as UCX and OB1 (Obiwan). This study focuses<br>mainly on pml/ob1, as the OB1 component utilize the BTL framework for the actual data<br>movement while the OB1 itself provides high-level operation such as message matching,<br>message pipelining, and sequencing. By utilizing the BTL framework, OB1 becomes the<br>component that provides the high-level algorithm design and allowing other developers to<br>easily plug their network implementation with only basic data movement functionality.</p>
<h1 id='170' style='font-size:20px'>Byte Transporting Layer (BTL)</h1>
<p id='171' data-category='paragraph' style='font-size:16px'>The interaction between the BTL and pml /ob1 is illustrated in Figure 2.2. The Byte<br>Transporting Layer is a framework with only basic data movement capabilities such as<br>memory allocation, send, and read for message completion. It is designed to work with<br>a higher level framework such as pml /ob1. The BTL itself does not have the context of any<br>message that it is sending or receiving, and it will let the higher level handle the completion of<br>the message. The simplicity of the BTL framework allows multiple network hardware vendors<br>to create their basic component and easily integrate it into Open MPI without implementing<br>the entire process of MPI communication such as message matching and ordering guarantees.<br>The example of the components in the BTL frameworks are: btl/tcp (socket communication),<br>btl/ugni (Cray's GNI), btl/ openib (ibverbs), btl/vader (shared-memory communication).</p>
<h1 id='172' style='font-size:22px'>2.3.2 Progress Engine</h1>
<p id='173' data-category='paragraph' style='font-size:14px'>The MPI standard does not provide the explicit API for progressing operations, but most<br>underlying network protocol requires an explicit progress routine. The MPI implementation,<br>as a middleware between the network protocols and the user, has to provide a solution to<br>comply with the standard. Most MPI implementations address the progression by creating<br>a centralized routine-the progress engine for progressing the communication of network<br>protocols and also internal MPI events. The standard mentioned that the user can expect the</p>
<footer id='174' style='font-size:14px'>16</footer>
<figure><img id='175' style='font-size:20px' alt="MPI Send
MPI API
PML API pml_send
pml_ob1_send pml ucx send
PML Components
BTL API
btl send
(With PML/OB1)
BTL Components
sendv ibv_send memcpy
(tcp) (openib) (vader)
Hardware
Ethernet Verbs devices Local Memory" data-coord="top-left:(207,465); bottom-right:(1090,1157)" /></figure>
<p id='176' data-category='paragraph' style='font-size:16px'>Figure 2.2: Interaction between BTL framework and pml/ob1 in Open MPI.</p>
<footer id='177' style='font-size:14px'>17</footer>
<p id='178' data-category='paragraph' style='font-size:16px'>progression of their asynchronous operation with every call into the MPI library. However,<br>the decision to invoke the call to the progress engine is entirely up to the MPI implementation.<br>Usually, in the blocking operation such as MPI_Recv or MPI_Wait the MPI implementation<br>will keep invoking the progress engine until the associated operation is completed.</p>
<p id='179' data-category='paragraph' style='font-size:16px'>For Open MPI, every component registers their progress routine to the centralized<br>progress engine (OPAL progress). When invoked, the progress engine will execute every<br>registered routine. Figure 2.3 illustrates the example of components who register their<br>progress routine to the Open MPI progress engine. For example, the BTL has its network<br>polling or reading for message completion registered to the progress engine. Since the<br>progress engine is involving handling the message completion event, it plays a crucial role<br>in the message receiving path of MPI, as the matching process is mostly performed in this<br>part. The details of the message matching process is discussed in the next section.</p>
<p id='180' data-category='paragraph' style='font-size:16px'>In multi-threaded environments, Open MPI ensures thread safety by serializing the call<br>into the progress engine to eliminate the possibility of any race condition that might occur.<br>The serialization is implemented by a process-wide lock, progress lock with the pthread<br>condition variable to synchronize between threads.</p>
<h1 id='181' style='font-size:20px'>2.3.3 Matching Process</h1>
<p id='182' data-category='paragraph' style='font-size:14px'>For two-sided communication, MPI offers tag matching in the standard as a means to pair<br>a sent message with an expected reception, instead of working with a simple stream of data.<br>The tag matching provides the user with better control over their communication, as they<br>can use the tag to distinguish between different messages with the same size or use them as a<br>label for each message. The matching process can occur at two points; When the user posts<br>a receive (through MPI_Recv, MPI_Irecv and other variations), and when the MPI process<br>extracts a message from the network. Therefore, the matching process involves two queues,<br>expected and unexpected.</p>
<p id='183' data-category='paragraph' style='font-size:16px'>Figure 2.4 shows the matching process implementation. In the case of the user posting<br>receive, the MPI will search for the message in the unexpected queue, a buffer queue when<br>the message arrives before the receiver posts the receive. If there is a match, the receive<br>operation is complete immediately. If not, the request will be added to the expected queue</p>
<footer id='184' style='font-size:18px'>18</footer>
<figure><img id='185' style='font-size:14px' alt="Open MPI Progress Engine
(opal_progress)
Framework
BTL progress coll progress pml progress
Component
vader progress tuned progress ob1 progress
tcp progress libnbc progress ucx progress
ugni progress" data-coord="top-left:(187,270); bottom-right:(1094,667)" /><figcaption id='186' style='font-size:18px'>Figure 2.3: Every component registers their progress routine to the progress engine.</figcaption></figure>

<figure><img id='187' style='font-size:14px' alt="User MPI
Incoming Message
Post receive
from network
Not found
Unexpected Queue Expected Queue
*messages *requests
found found
Request is complete." data-coord="top-left:(306,958); bottom-right:(968,1329)" /><figcaption id='188' style='font-size:20px'>Figure 2.4: Matching Process implementation</figcaption></figure>

<footer id='189' style='font-size:16px'>19</footer>
<p id='190' data-category='paragraph' style='font-size:14px'>to match with incoming message later. On the other hand, in case of receiving an incoming<br>message from the network, the MPI implementation will try to match the message's tag with<br>the request in the expected queue. If there is a match, then the matched request is marked<br>as completed. If not, the message will be added to the unexpected queue for matching with<br>the request later.</p>
<p id='191' data-category='paragraph' style='font-size:14px'>The MPI standard also guarantees that the message will be matched in the FIFO<br>ordering. In short, the MPI will match the message by the order of the posting of the<br>reception, as well as the order of the sending. For example, if the message has the same tag,<br>the first receive will always get matched with the first send, the second receive with the second<br>send, and SO on. This guarantee provides the simplicity for the user to program. However,<br>there is no such guarantee from the network level as to whether implementing the ordering<br>from the hardware level will hinder the overall hardware performance capacity. Thus, the<br>MPI implementations have to provide the software solution for the ordering guarantee to<br>the user. The message ordering implementation is different for each MPI implementation,<br>but the general idea is to issue a sequence number for each message. The sender will include<br>the sequence number with the message header, and the receiver can check if the message's<br>sequence number is expected. By imposing the monotonically increasing sequence number on<br>both sides, the MPI implementation can ensure that the message is matched in the described<br>FIFO order.</p>
<p id='192' data-category='paragraph' style='font-size:14px'>Figure 2.5 illustrates the matching process implementation with the FIFO ordering<br>guarantee by utilizing the message sequence number. When the receiver receives an<br>incoming message with non-FIFO ordering (incorrect sequence number) from the network,<br>the MPI buffers the message and adds it to the out-of-sequence queue without performing<br>any matching operation on it. Every time the MPI successfully matches a message to a<br>request, the anticipated sequence number changes. Before returning to the user, the MPI<br>implementation will try to search for the message with the anticipated sequence number in<br>the out-of-sequence queue. If the message with the anticipated sequence number is found,<br>the usual matching process is performed on the message. There can be two outcomes: if the<br>user posted receive for that message, the request associated with the receive is marked as<br>completed; if not, the message gets moved into the unexpected queue.</p>
<footer id='193' style='font-size:18px'>20</footer>
<figure><img id='194' style='font-size:14px' alt="User MPI
Incorrect sequence number
Incoming Message
Post receive
from network
correct sequence number
Not found
found
Unexpected Queue Expected Queue Out-of-sequence Queue
*messages *requests
found found
Request is complete.
Check for next expected number" data-coord="top-left:(223,163); bottom-right:(1112,523)" /><figcaption id='195' style='font-size:20px'>Figure 2.5: Matching process implementation with ordering guarantees</figcaption></figure>

<p id='196' data-category='paragraph' style='font-size:18px'>Usually, buffering the message in the unexpected queue or out-of-sequence queue is a<br>costly operation, as the MPI implementation has to allocate proper memory to store the<br>message in the middle of the time-critical message extraction process, but the occurrence is<br>expected to be very minimal as the network devices rarely rearrange the send order under<br>normal circumstances. Another interesting metric for the matching process is the unexpected<br>queue length, as the matching operation is essentially a serial queue search, where the cost of<br>searching the queue grows with the length of the queue itself. Therefore, applications with a<br>high volume of unexpected messages (not pre-posting receives) will be susceptible to longer<br>matching time.</p>
<p id='197' data-category='paragraph' style='font-size:16px'>There are more complications in the multi-threaded scenario as the matching process has<br>to be serialized, at least per communicator, to ensure correctness. The critical parts, such<br>as adding or removing an object from a queue, cannot be performed concurrently. In the<br>context of MPI implementation, two threads can post a receive at the same time, or one<br>thread is inside the MPI progress engine receiving the incoming message while another thread<br>is posting the receive. The MPI implementation usually protects the matching process with<br>a process-wide lock or matching lock to serialize the access to all queues.</p>
<footer id='198' style='font-size:18px'>21</footer>
<h1 id='199' style='font-size:20px'>2.4 Literature Reviews</h1>
<p id='200' data-category='paragraph' style='font-size:14px'>Multiple studies have been conducted investigating ways to improve the efficiency of multi-<br>threaded MPI. There are several moving parts contributing to the abysmal performance of<br>multi-threaded MPI implementations. This section discusses several research topics related<br>to multi-threaded MPI communication and the efforts to improve its performance from<br>several perspectives.</p>
<h1 id='201' style='font-size:18px'>Threading Frameworks and HPC</h1>
<p id='202' data-category='paragraph' style='font-size:16px'>There are research interests in threading performance optimization in HPC other than the<br>use of the classic Portable Operating System Interface (POSIX) threading framework such<br>as pthread Lewis and Berg (1998). OpenMP Dagum and Menon (1998) is the threading<br>framework that provides the high-level abstraction that is user friendly but still based on<br>the fork-join programming model. However, the OpenMP specifications in recent years added<br>more support for current trends in HPC such as tasks and job-stealing design. From the<br>large-scale parallelism perspective, Wheeler et al. (2008) proposed Qthread, a lightweight,<br>portable threading framework, which is more suitable for the HPC environment with smaller<br>memory footprints per thread, with lightweight thread control and synchronization while<br>not relying on any specific hardware or the platform capability, allowing for more scalability<br>and portability of the multi-threaded HPC applications. In the same year, Intel released<br>its Thread Building Block (Intel TBB) Pheatt (2008) which presented the similar idea of<br>threading and its portability by providing its own user-level threading runtime that evaluates<br>the system it is currently running on, and performs the load balancing for threads accordingly.</p>
<br><p id='203' data-category='paragraph' style='font-size:14px'>In recent years, MassiveThreads Nakashima and Taura (2014) presented the threading<br>framework with the same API as pthread but offer more delicate thread management design<br>for better load balancing and workload prediction to avoid the high cost of unnecessary<br>context switching, resulting in lower overhead compared to the existing solution such as<br>Qthread. Argobots Seo et al. (2017) are refining the approach of the lightweight threading<br>framework, which directly focuses on the HPC usage while providing the high-level threading</p>
<footer id='204' style='font-size:16px'>22</footer>
<p id='205' data-category='paragraph' style='font-size:14px'>capability on its own, or it can be used as the low-level threading infrastructure under other<br>threading frameworks.</p>
<br><p id='206' data-category='paragraph' style='font-size:16px'>The works presented in this section show high interest in incorporating the multi-thread<br>environment into the HPC landscape with an increasing number of CPU cores per chip. This<br>naturally calls for the combination of MPI and threads (MPI+threads) to handle larger scale<br>applications where MPI is used for inter-node communication, while using threads to perform<br>the computation task locally on the node, and spurs interests in optimizing thread support<br>in the MPI.</p>
<p id='207' data-category='paragraph' style='font-size:20px'>Lack of Interoperability Between the MPI and the Threading Frameworks</p>
<p id='208' data-category='paragraph' style='font-size:20px'>The MPI standard does not provide an API for the MPI implementation to exchange the<br>information with the threading frameworks of the application. The MPI implementation<br>cannot differentiate between threads that make calls into the MPI library, thus limiting<br>the optimization strategies that it can employ. Si et al. (2014) proposed interoperability<br>between the de facto OpenMP threading runtime and the MPI implementation to share<br>the information of the threads between the application and the MPI layer in many-core<br>architectures. When equipped with threading information from the threading framework,<br>MPI can incorporate the idling application threads for internal communication without user<br>intervention, allowing for better computing resources utilization.</p>
<p id='209' data-category='paragraph' style='font-size:16px'>On the other hand, there is a movement to extend the MPI standard to address this<br>interoperability problem. The study of Sridharan et al. (2014) and Dinan et al. (2014) shows<br>interest in creating multiple endpoints per MPI process for multiple threads to communicate.<br>This is a step towards better interoperability by providing the user with a standardized way to<br>provide hints to the MPI implementation for better optimization. The MPI implementation<br>can be utilizing this information for better resource allocation internally. Unfortunately,<br>at the time of this study, the MPI forum has not yet accepted the proposal, but the MPI<br>implementation can still offer the non-standardized way for the user to provide hints such as<br>using environment variables, among other means. Nonetheless, I believe that interoperability<br>between the MPI implementation and the threading framework is desperately needed for<br>better performance optimization of multi-threaded MPI.</p>
<footer id='210' style='font-size:20px'>23</footer>
<h1 id='211' style='font-size:18px'>Resources Contention in Multi-threaded MPI</h1>
<p id='212' data-category='paragraph' style='font-size:14px'>First, the resource contention in multi-threaded environment: it has been identified as<br>a major roadblock that prevents the MPI implementation from harnessing true thread<br>parallelism. The MPI implementation has to create a critical section -a section that can<br>be executed by a single thread at a time, to prevent the race condition that might occur<br>when multiple threads are making updates to the same memory location. The race condition<br>from threads can affect the correctness of the communication, or even escalate to corrupt<br>the state of the MPI software and crash the whole application. Thus, the critical section<br>also becomes a funnel that mitigates the potential performance gain from thread parallelism.<br>Balaji et al. (2008) study multiple granularities of the critical section in MPI by simulations<br>with MPI_PROC_NULL (no actual data movements), and suggested that coarse-grain critical<br>sections such as global lock even for a short duration per-thread, per-access can create<br>bottlenecks that significantly affect the overall communication performance. The study also<br>suggested MPI implementations should shift towards per-object lock or even lockless data<br>structure to avoid the massive lock contention.</p>
<p id='213' data-category='paragraph' style='font-size:14px'>Amer et al. (2015) suggested another approach to minimize the lock contention by<br>creating a systematic mechanism to assign the resources to a thread. Their study imposes<br>priority to threads, and managing the load balancing between threads by several strategies.<br>The study suggested that multithreading can benefit from a well-designed load balancing<br>algorithm. However, it has been noticed that the MPI standard does not provide any<br>interoperability between the MPI and the threading framework, which might render the<br>load balancing more challenging for the MPI implementation. In another study, Goodell<br>et al. (2010) illustrate the impact of an often overlooked aspect: the reference counting<br>of the object. Reference counting is a common practice to track the usage of a shared<br>object by a simple counter addition and subtraction. Once the reference count of an object<br>becomes zero, the object is marked for garbage collecting purposes. However, the cost of<br>reference counting increases drastically in multi-threaded environments, as simple addition<br>becomes an atomic operation. Goodell's study proposes a design to reduce the cost of the<br>reference counting and demonstrates its impact on overall MPI communication. Dozsa et al.</p>
<footer id='214' style='font-size:18px'>24</footer>
<p id='215' data-category='paragraph' style='font-size:14px'>(2010) proposed a design of multi-channel communication for MPI; the MPI implementation<br>creates multiple channels for communication with different peers, allowing multiple threads<br>to perform multiple communications in parallel. Their study paves the way for achieving true<br>thread concurrency in MPI implementation, and suggested that the MPI implementation has<br>to implement a way to extract the incoming message in parallel. The study uses a parallel<br>receive queue for multiple threads to drain at the same time; the performance evaluation in<br>their study claims good performance scaling with an increasing number of threads.</p>
<p id='216' data-category='paragraph' style='font-size:14px'>There has been a trend to completely avoid resource contention by delegating every<br>communication to a dedicated communication thread. By using a single thread to<br>communicate, the application does not have to initialize the MPI library in multi-threaded<br>mode, thus removing all the cost of thread safety that comes with it. There are several ways<br>to achieve this model. The user can manually program their application accordingly, or<br>there can be a middle layer between the user and MPI such as Vaidyanathan et al. (2015)'s<br>study. Vaidyanathan proposed software offloading, intercepting every MPI call from user<br>threads, and funneling them into a dedicated communication thread via lockless command<br>queue. While the study cannot fully utilize the thread parallelism for communication, it<br>shows significant improvement over the current coarse-grain critical section design in some<br>MPI implementations. Grant et al. (2015) proposed another approach to avoid the resource<br>contention by accumulating smaller messages from multiple threads into a large buffer, and<br>use a single thread to perform the communication, utilizing the highly sophisticated message<br>pipelining mechanism on the larger message to achieve better performance while avoiding<br>the unnecessary contention. The work shows an impressive performance boost; however, it<br>requires user-level involvement in the initialization stage.</p>
<h1 id='217' style='font-size:20px'>Progress Threads</h1>
<p id='218' data-category='paragraph' style='font-size:14px'>Another trend in utilizing threads in MPI is the progress thread. The main attraction of<br>the progress thread design is computation and communication overlap. There is a common<br>misconception related to asynchronous communication in MPI, as the users expect the MPI<br>to progress the communication in the background. In reality, the MPI has to explicitly<br>progress the network through different protocols to get completion and MPI itself cannot</p>
<footer id='219' style='font-size:16px'>25</footer>
<p id='220' data-category='paragraph' style='font-size:14px'>do that in the background. Currently, for MPI to progress the communication, the user<br>has to make a call into the MPI library to give the chance for the MPI implementation to<br>execute the progress engine. The progress thread approach is a design where there is a thread<br>executing the progress engine in the background for the MPI implementation to progress the<br>outstanding communication while the user thread is executing application code, providing<br>the overlap between the two.</p>
<p id='221' data-category='paragraph' style='font-size:14px'>That being said, the progress thread approach often comes with the design question of<br>where the MPI implementation should bind the progress thread. If the progress thread<br>shares the physical CPU core with the application, it takes crucial computation power<br>away from the application; but if the progress thread is bound to a dedicated physical<br>core, there will be no interference with the application, but every MPI process has to<br>take additional CPU cores and ultimately cut the computation power of the application in<br>half. Hoefler and Lumsdaine (2008) experimented with multiple strategies of progress thread<br>design and proposed a shared core design with low performance impact on the application<br>while providing a good percentage of overlap. Wittmann et al. (2013) uses the standardized<br>PMPI interface to implement the progress thread from outside of the MPI implementation,<br>allowing a portable progress thread implementation for communication and IO overlap. Lu<br>et al. (2015) proposed a design to utilize user-level threads as a temporary progress thread,<br>circumventing the need for MPI implementation to spawn the progress thread by itself<br>and avoid the resource management problem entirely. The work of Potluri et al. (2010)<br>demonstrates the application-level benefit from communication overlap in real-world seismic<br>modeling applications.</p>
<h1 id='222' style='font-size:20px'>Matching Process</h1>
<p id='223' data-category='paragraph' style='font-size:14px'>The matching process is another critical piece of the two-sided communications infrastructure<br>(send and receive), which is the bedrock of the MPI communication. While this study does<br>not contribute to the topic of matching process optimization, multiple challenges in this study<br>stem from the current design of the matching process. This section presents the efforts from<br>the HPC community in optimizing the matching process by improving its efficiency and<br>releasing synchronization constraints.</p>
<footer id='224' style='font-size:16px'>26</footer>
<p id='225' data-category='paragraph' style='font-size:14px'>Brightwell et al. (2005) and Ferreira et al. (2018) present the characteristic of real-world<br>applications regarding the usage of the MPI matching queues (expected and unexpected<br>queues) and its effect on the overall performance. Brightwell suggested that one of the<br>interesting metrics is the queue length, as the cost for the MPI implementation to search<br>through the queue increases linearly with it. Since the matching process is mandatory for<br>two-sided communication, speeding the process up will be beneficial to both single- and<br>multi-threaded modes of the MPI implementations.</p>
<p id='226' data-category='paragraph' style='font-size:14px'>There are multiple studies of matching process optimization in the past. Flajslik et al.<br>(2016) suggested a binned matching algorithm to alleviate the contention of the matching<br>process. The study presents a significant speedup over the traditional matching process and<br>suggested that the approach can be easily adapted for multi-threaded environments as they<br>are more fine-grained and suitable for per-object lock. Schonbein et al. (2018) proposed<br>the use of Intel vector instruction to implement a fuzzy matching algorithm for the global<br>matching queue of MPI. This, in turn, allows multiple messages matching at the same time<br>and shows vast performance improvements. Bayatpour et al. (2016) proposed an adaptive<br>algorithm for the tag matching to use different tag matching designs for different workloads.</p>
<p id='227' data-category='paragraph' style='font-size:14px'>Another interest in optimizing the matching process is the possibility to offload it to<br>the hardware, relieving the MPI of the matching duty entirely, and supposedly speeding<br>up the entire communication process. The work of Underwood et al. (2005) pioneered the<br>concept of specialized hardware accelerated queue in MPI. Moreover, Hemmert et al. (2007)<br>and Gupta and Abels (2006) demonstrate the benefit of moving the matching process entirely<br>into the network hardware itself. Currently, we can see several high-performance network<br>hardware vendors such as Mellanox, Cray, and Intel incorporate the tag matching capability<br>into their hardware design. While offloading the tag matching process to the hardware can<br>be beneficial for the MPI implementations, it still poses some set of limitations such as the<br>lack of the possibility to cancel the messages correctly, or the ability to split the matching<br>process for different MPI communicators.</p>
<footer id='228' style='font-size:18px'>27</footer>
<h1 id='229' style='font-size:18px'>One-Sided Communication</h1>
<p id='230' data-category='paragraph' style='font-size:14px'>This dissertation focuses mainly on two-sided communication, but also touches the topic<br>of multi-threaded one-sided communication in Chapter 5. The past studies on one-sided<br>communication emerged as soon as the MPI-2.1 standard was published. Barrett et al.<br>(2007) studied several approaches to implementing the one-sided communication support<br>in Open MPI. Barrett's study suggested that if the network hardware is equipped with<br>remote memory access capability, one-sided communication can achieve higher bandwidth<br>and lower latency than the two-sided communication in some cases. In the same year, Gropp<br>and Thakur (2007) studied one-sided communication performance on various platforms<br>with various MPI implementations, and their study also gives the conclusion in the same<br>direction as Barrett's. However, the MPI-2.1 standard poses some limitations for one-sided<br>communication as it is written vaguely. In 2009, Tipparaju et al. (2009) 's study proposed the<br>improvements in one-sided communication to the MPI standard which resulted in MPI-3.0<br>standard. Kumar and Blocksome (2014) implemented the MPI-3.0 one-sided communication<br>on Blue Gene/Q computer and evaluated the performance. Their study concluded that<br>MPI-3.0 one-sided communication has lower latency than the older MPI-2.2 standard while<br>performing at the same level as two-sided communication. In their remarks, they also<br>show interest in extending the support for multi-threaded one-sided communication and<br>utilizing the internal progress thread to increase communication overlap. In 2016, where<br>several MPI implementations are fully standard compliance, Dinan et al. (2016) presents<br>the implementation and performance evaluation of the MPI-3 one-sided communication and<br>suggested that the new MPI-3.1 standard allow MPI implementation to be fully equipped,<br>and fully utilize the hardware capabilities in remote memory access.</p>
<br><p id='231' data-category='paragraph' style='font-size:14px'>Recently, there is increasing interest in multi-threaded one-sided communication. First,<br>the effort to create a multi-threaded one-sided MPI communication benchmark by Dosanjh<br>et al. (2016) signals the interest of MPI developers in pushing for multi-threaded RDMA<br>support. The follow-up work includes Hjelm et al. (2018) where the authors investigate<br>several techniques to improve one-sided communication in multi-threaded scenarios. Hjelm<br>is also one of the collaborators of this work.</p>
<footer id='232' style='font-size:18px'>28</footer>
<h1 id='233' style='font-size:18px'>Chapter 3</h1>
<h1 id='234' style='font-size:20px'>Measuring MPI Performance</h1>
<h1 id='235' style='font-size:16px'>3.1 Overview</h1>
<p id='236' data-category='paragraph' style='font-size:14px'>This chapter presents my effort to expand the number of powerful tools for the MPI<br>developers to evaluate their MPI implementation. I introduce my novel Multirate<br>benchmark: a highly flexible benchmark, capable of stressing multiple performance points<br>of an MPI implementation. The Multirate benchmark is designed to provide fast assessment<br>and comparison between MPI in a normal process-to-process communication and thread-to-<br>thread communication. This chapter discusses the motivation and design of the Multirate<br>benchmark, and shows that my tool is capable of exposing the optimal and sub-optimal<br>point in the MPI implementations. I demonstrate Multirate capability by evaluating three<br>current state-of-the-art MPI implementations and discuss my findings.</p>
<h1 id='237' style='font-size:16px'>3.2 Introduction</h1>
<p id='238' data-category='paragraph' style='font-size:14px'>Several MPI implementations are available today, with varying capabilities and support<br>for lesser-used MPI features. While it is tempting to make a distinction between vendor<br>implementations and open-source implementations, the software evolution in HPC has led<br>to only two major flavors, MPICH and Open MPI- both of which are open source<br>surrounded by a series of derivative implementations with small differences compared with<br>their underlying open-source versions.</p>
<footer id='239' style='font-size:14px'>29</footer>
<p id='240' data-category='paragraph' style='font-size:16px'>Current state-of-the-art MPI implementations are categorized into two groups, vendor-<br>specific MPI with platform optimization and aftermarket support such as Intel MPI,<br>Cray MPICH, or IBM Spectrum MPI, and open-source implementations such as MPICH,<br>MVAPICH, and Open MPI where the community voluntarily maintains and contributes.</p>
<p id='241' data-category='paragraph' style='font-size:16px'>Each MPI implementation differs in design, as MPI developers are free to optimize their<br>implementations. A particular MPI implementation might perform well in one aspect but<br>not in others. Moreover, with different communication patterns required for each type of<br>application, it is essential for the application developers to know which MPI implementation<br>gives the best performance for their application.</p>
<p id='242' data-category='paragraph' style='font-size:16px'>Benchmarking is one of several approaches that application developers can leverage to<br>quickly evaluate the performance of their MPI implementation and tailor it to suit their<br>needs. It has the added benefit of enabling MPI developers to validate and assess the<br>performance impact of changes made to their implementation and gives them the ability to<br>compare performance between released versions.</p>
<br><p id='243' data-category='paragraph' style='font-size:16px'>There are several approaches to benchmarking the performance of MPI implementations.<br>One is a simple communication test where the benchmark uses basic patterns like one-to-one.<br>By doing so, the benchmark can show the general behavior of the MPI implementation, but<br>the communication patterns in the benchmark may not be similar to the application, which<br>can be misleading in some cases.</p>
<p id='244' data-category='paragraph' style='font-size:14px'>Another approach is to imitate a real-world workload by creating a small subroutine that<br>includes real computation and an actual communication pattern from the application itself.<br>This approach can provide a good performance assessment of the interaction between the<br>application and the MPI implementation as the network engine. This approach, however,<br>puts a burden on the application developer to create a benchmark that reflects their workload<br>and communication patterns, which would change for each application and is a non-trivial<br>task.</p>
<p id='245' data-category='paragraph' style='font-size:16px'>The MPI standard offers many communication functionalities, and benchmarking every<br>aspect of MPI would be difficult. This work only focuses on point-to-point communication,<br>as this is the most commonly used feature in MPI applications and also serves as a building<br>block for more complicated schemes, like collective operations Luo et al. (2018). The goal is</p>
<footer id='246' style='font-size:20px'>30</footer>
<p id='247' data-category='paragraph' style='font-size:14px'>to offer performance measurements at the point between a simple communication test and<br>an application-specific benchmark. I build a tool that is flexible in communication patterns,<br>workload, and mode of operations, which can be beneficial to both application and MPI<br>developers in evaluating MPI implementations.</p>
<p id='248' data-category='paragraph' style='font-size:14px'>I propose Multirate, a flexible benchmark that offers multiple communication patterns<br>that can be mapped to real-world application needs using an adjustable workload to enable<br>developers to perform early assessments of their MPI implementations. Multirate offers<br>three modes of operation processes, threads, and hybrid- -and enables a quick comparison<br>of performance between the three modes. The results can be used to identify possible<br>bottlenecks in MPI implementations, which can be highly beneficial to MPI developers<br>looking to improve multi-threaded support for MPI.</p>
<h1 id='249' style='font-size:20px'>3.3 Background</h1>
<h1 id='250' style='font-size:18px'>3.3.1 Metrics</h1>
<p id='251' data-category='paragraph' style='font-size:14px'>For MPI performance metrics, we are usually interested in bandwidth, latency, and message<br>rate, which most MPI benchmarks-including Multirate -will be measuring. While the<br>instrumentation for every metric mentioned is very similar, each metric can be more or less<br>critical depending on a given application.</p>
<br><p id='252' data-category='paragraph' style='font-size:14px'>Many scientific applications, such as fast Fourier transform (FFT) or a general distributed<br>machine learning framework, usually perform matrix transpose operations with data transfer<br>to their peers after each iteration. The communication workload in this type of application<br>is usually substantial, making it reliant on the network bandwidth, as it reflects the capacity<br>of the network stack. However, for large messages, the long transfer time often overshadows<br>the overhead from MPI. Most MPI implementations should be able to provide comparable<br>performance.</p>
<p id='253' data-category='paragraph' style='font-size:14px'>On the other hand, some applications rely heavily on the small to medium sized messages.<br>However, it is very challenging for MPI to reach peak network bandwidth with small sized<br>messages, as the actual transfer time is short and effectively makes MPI overhead the</p>
<footer id='254' style='font-size:16px'>31</footer>
<p id='255' data-category='paragraph' style='font-size:14px'>bottleneck in the overall communication time. Since additional MPI overhead is added with<br>each message, sending multiple small messages will incur more overhead than sending one<br>large message. The ability to transfer multiple smaller messages simultaneously to hide the<br>overhead and increase the communication throughput has been one of the main attractions<br>of multi-threaded MPI (Figure 3.1). Message rate is the metric that involves the size of the<br>message and might be more suitable for measuring performance in some cases. It can also<br>be used to evaluate the MPI overhead directly.</p>
<p id='256' data-category='paragraph' style='font-size:14px'>Latency is another important metric for most applications, as the faster the message is<br>received, the faster the application can move into the next stage of computation, thereby<br>reducing overall run time. Latency differences between MPI implementations can often be<br>used to compare the quality of the optimization or the algorithm used in each of case.</p>
<h1 id='257' style='font-size:20px'>3.3.2 Workloads</h1>
<p id='258' data-category='paragraph' style='font-size:14px'>Communication performance can vary drastically under different workloads. Usually, we<br>can describe the communication workload in two ways: (1) the message size and (2) the<br>number of messages. The message size is an extremely important parameter, used by most<br>MPI implementations as a trigger for different low-level communication protocols (eager VS.<br>rendezvous). To be able to accurately quantify the protocol switching points, most MPI<br>benchmarks offer presets for message sizes or make them freely adjustable in some cases.</p>
<p id='259' data-category='paragraph' style='font-size:14px'>The number of concurrent messages is another factor that can affect MPI communication<br>performance. Having a high number of messages posted at the same time might stress<br>MPI's internal message handling (matching costs, the fairness of receives, the fairness of<br>progress) and load balancing capabilities. Some MPI benchmarks offer an adjustable number<br>of concurrent messages in the form of "window size" or the number of posted messages per<br>iteration. Using both message size and window size to adjust the workload can offer a more<br>comprehensive assessment of MPI performance.</p>
<footer id='260' style='font-size:16px'>32</footer>
<figure><img id='261' style='font-size:14px' alt="One large message, in multiple chunks
Multiple smaller messages
Multiple smaller messages, multiple threads
Time
Data transfer Matching Overhead Multi-threaded Overhead" data-coord="top-left:(275,191); bottom-right:(1033,727)" /></figure>
<p id='262' data-category='paragraph' style='font-size:16px'>Figure 3.1: The use of multiple threads to increase communication throughput.</p>
<h1 id='263' style='font-size:20px'>3.3.3 Communication Patterns</h1>
<p id='264' data-category='paragraph' style='font-size:16px'>Most MPI point-to-point benchmarks usually perform communication between two MPI<br>ranks. While this approach certainly gives some insightful performance data on trivial<br>communication patterns, it rarely represents a realistic communication pattern that is used<br>by complex parallel applications.</p>
<p id='265' data-category='paragraph' style='font-size:16px'>Communication between two MPI ranks with smaller messages might not be enough to<br>achieve peak hardware bandwidth. However, MPI users can mitigate this problem by using<br>multiple ranks or threads to send messages simultaneously, thereby hiding the MPI overhead<br>by having multiple messages ready to be sent as soon as the network finishes the transfer of<br>the previous message. However, existing MPI benchmarks often do not offer multiple ranks<br>and one-to-one communication patterns in their testing.</p>
<p id='266' data-category='paragraph' style='font-size:16px'>Additionally, non-deterministic, thread-based task-based applications, such as MAD-<br>NESS Thornton et al. or rootSIM Pellegrini et al. (2011), and task- based runtimes, such<br>as StarPU Augonnet et al. (2011) or PaRSEC Bosilca et al. (2013), rely on coordination</p>
<footer id='267' style='font-size:18px'>33</footer>
<p id='268' data-category='paragraph' style='font-size:14px'>messages between peers, which can exhibit dynamic communication patterns between stages<br>of the application and between different executions. A single MPI rank might be the target<br>of messages from multiple peers at the same time, or be one of the sources of messages to<br>a particular target. Similarly, most collective operations also make use of similar point-to-<br>point communication behavior. This behavior can be generalized into simple communication<br>patterns like one-to-many or many-to-one.</p>
<p id='269' data-category='paragraph' style='font-size:14px'>One-to-many and many-to-one (Figure 3.2) are interesting communication patterns,<br>as they can be used to test MPI implementations under the asymmetric workload often<br>presented in the real-world application. For example, the stencil, or the halo neighbor<br>exchange, is commonly used in scientific applications. These communications can be mapped<br>into one-to-many and followed by many-to-one. An imbalance between the message injection<br>and extraction can become a major bottleneck for the overall communication: the sender<br>might inject the message with a higher rate than the receiver can effectively extract from<br>the network layers, introducing significant delays in message delivery to the user level.<br>Performing tests on these patterns can help in exposing the strengths and the weaknesses of<br>the MPI implementations.</p>
<h1 id='270' style='font-size:20px'>3.3.4 Threading in MPI</h1>
<p id='271' data-category='paragraph' style='font-size:16px'>Ideally, running on the same hardware, MPI with and without threading support should<br>have comparable performance. However, in practice, the cost of serialization from MPI</p>
<table id='272' style='font-size:18px'><tr><td></td><td></td></tr><tr><td>One-to-many</td><td>Many-to-one</td></tr></table>
<caption id='273' style='font-size:16px'>Figure 3.2: One-to-many and many-to-one communication pattern</caption>
<footer id='274' style='font-size:16px'>34</footer>
<p id='275' data-category='paragraph' style='font-size:16px'>often overshadows the gained benefits from the implicit intra-node communication. As a<br>result, not many applications are adopting a full MPI threading model, despite significant<br>interest.</p>
<p id='276' data-category='paragraph' style='font-size:16px'>The early adoption of the multi-threaded MPI in scientific applications comes after the<br>MPI standard officially defined the multi-threaded environment support in 2008. Shan et<br>al. studied the performance of the MPI threading environment for a large-scale molecular<br>simulation, NWChem Valiev et al. (2010), and concluded that a hybrid combination of<br>processes and threads provides the best performance in this particular application Shan<br>et al. (2015). This evaluation process can be done on a much smaller scale by utilizing a<br>flexible benchmark to represent similar communication patterns without computation.</p>
<p id='277' data-category='paragraph' style='font-size:14px'>Currently, there are several multi-threaded MPI benchmarks available in the market.<br>Most of the benchmarks only offer a one-to-one communication pattern and perform the<br>communication in the same way as they do between ranks. The significant difference between<br>process and thread mode in the resources allocated for the operations also needs to be<br>considered. In process mode, each process allocates the resource instance, while thread<br>mode usually spawns multiple threads from a single process, and threads are likely to race<br>against each other for access to limited resources, causing a slowdown.</p>
<h1 id='278' style='font-size:20px'>3.4 Existing Benchmarks</h1>
<p id='279' data-category='paragraph' style='font-size:16px'>There are several MPI benchmarks already available. NetPIPE Snell et al. (1996) is a set<br>of tools used to measure communication performance. NetPIPE-MPI offers one of the most<br>efficient MPI performance tests for point-to-point communication between two MPI ranks.<br>It also offers bare-bones performance analysis for latency and bandwidth. However, the<br>limitation of NetPIPE is that it only tests communication between two MPI ranks. Sandia<br>microbenchmark Lawry et al. (2002) offers MPI message rate measurement with variable<br>message size, window size, and number of peers, but only offers one communication pattern.</p>
<br><p id='280' data-category='paragraph' style='font-size:18px'>Aside from straightforward tests, several benchmarks (e.g., GRID Boyle et al. (2015)<br>or Parallel Research Kernels der Wijngaart (2016)) offer more specific communication</p>
<footer id='281' style='font-size:18px'>35</footer>
<p id='282' data-category='paragraph' style='font-size:14px'>patterns that are commonly used by scientific applications, including neighbor commu-<br>nication (stencil), parallel matrix transpose, and a distributed linear algebra subroutine.<br>These specialized benchmarks could help application developers make early assessments<br>of their communication patterns by showing them the expected performance from MPI<br>implementations using a practical workload. Several applications from the US Department<br>of Energy's Exascale Computing Project provide proxy applications Proxy, which act as<br>miniature versions of a project to represent the workload, thereby enabling a more accurate<br>performance assessment.</p>
<p id='283' data-category='paragraph' style='font-size:14px'>With increasing interest in improving MPI performance in a many-thread environment,<br>several well-known threading benchmarks have emerged. Thread tests Thakur and Gropp<br>is one of the popular MPI multi-threaded benchmarks and offers a fundamental, point-to-<br>point send with the ability to increase the number of threads. OSU microbenchmark OSU<br>offers multiple measurements of point-to-point communications, including bandwidth, bi-<br>directional bandwidth, latency, and message rate with variable window size and multiple<br>communication pairs. However, in threading mode, it only offers latency testing.</p>
<h1 id='284' style='font-size:20px'>3.5 Multirate Benchmark</h1>
<p id='285' data-category='paragraph' style='font-size:14px'>The goal is to offer performance measurement at the point between a simple communication<br>test and an application-specific benchmark. Multirate is flexible and can adjust the mode<br>of operation (process, thread, and hybrid) and the size of the workload with various<br>communication patterns; this enables application developers to perform an early assessment<br>of their communication needs and provides a quick comparison between different settings for<br>the MPI developer to help him or her identify problems or bottlenecks in the implementation.</p>
<p id='286' data-category='paragraph' style='font-size:18px'>3.5.1 Communication Patterns</p>
<p id='287' data-category='paragraph' style='font-size:14px'>Multirate provides multiple communication patterns that can be used to map the application<br>behavior or used directly to test capability and identify bottlenecks of MPI implementations.<br>The patterns currently offered are Pairwise, one-to-many, many-to-one, and many-to-many.</p>
<footer id='288' style='font-size:16px'>36</footer>
<p id='289' data-category='paragraph' style='font-size:18px'>Pairwise (Figure 3.3) is one-to-one mapping from sender to receiver. The goal of this<br>pattern is to extract the best possible communication performance from MPI with the<br>balanced workload between message injection and extraction. However, for the small-<br>sized messages, using only a single one-to-one communication pair might not be able to<br>achieve the peak network bandwidth. Deploying multiple one-to-one communication pairs<br>concurrently to keep the network hardware occupied might be beneficial to the overall<br>bandwidth utilization.</p>
<p id='290' data-category='paragraph' style='font-size:18px'>many-to-one and one-to-many (Figures 3.4b and 3.4c) can be useful for detecting<br>bottlenecks in MPI implementations. For example, a single sender to multiple receivers<br>will test the capability of message injection from the sender while mitigating the bottleneck<br>from message extraction, as multiple receivers can split the receiving workload among them.<br>On the other hand, many-to-one overwhelms a single receiver with incoming messages from<br>different peers at the same time, solely testing the capability of message extraction. In<br>many-to-many (Figure 3.4a), the user can choose any arbitrary number of sender and receiver<br>entities. It is useful for identifying the optimal workload for particular MPI implementations.</p>
<h1 id='291' style='font-size:22px'>3.5.2 Communication Entities</h1>
<p id='292' data-category='paragraph' style='font-size:16px'>MPI process mapping can be different in multi-threaded mode, as the users are usually<br>advised to map one MPI process onto the entire node or socket (Figure 3.3) and let</p>
<figure><img id='293' style='font-size:14px' alt="Node 0 Node 1 Node 0 Node 1 Node 0 Node 1" data-coord="top-left:(267,1113); bottom-right:(1013,1419)" /><figcaption id='294' style='font-size:18px'>Figure 3.3: CPU core mapping to MPI ranks for pairwise pattern.</figcaption></figure>

<footer id='295' style='font-size:20px'>37</footer>
<figure><img id='296' style='font-size:14px' alt="a. b. C." data-coord="top-left:(270,220); bottom-right:(1013,552)" /></figure>
<p id='297' data-category='paragraph' style='font-size:20px'>Figure 3.4: All-to-all communication can be used to make sub-patterns, such as many-to-<br>many (a), many-to-one (b), and one-to-many (c).</p>
<p id='298' data-category='paragraph' style='font-size:16px'>the operating system schedule threads automatically. The communication entity is an<br>abstraction level that the Multirate benchmark uses to describe a communication body.<br>An entity can be mapped to a single MPI process or to a single thread.</p>
<br><p id='299' data-category='paragraph' style='font-size:18px'>Communication entity abstraction enables Multirate to perform the test in multiple<br>modes of operation, such as thread to thread communication (thread mode), process to<br>process communication (process mode), or combinations of thread to process communication<br>(hybrid mode).</p>
<p id='300' data-category='paragraph' style='font-size:18px'>Performance results from process mode and thread mode can be compared to demonstrate<br>the overhead from initializing MPI with full threading support, while hybrid mode can be<br>used to further pinpoint the performance degradation from threading support by fixing one<br>side to process entities and alternating between thread and process entities on the other side,<br>for comparison.</p>
<p id='301' data-category='paragraph' style='font-size:22px'>3.5.3 Communicator's Effect</p>
<p id='302' data-category='paragraph' style='font-size:16px'>From the user's perspective, the communicator is an MPI-defined abstraction to refer to a<br>group of MPI processes. In each communicator, the participating processes are assigned an<br>individual 'rank' number related to that particular communicator. The communicator is<br>used in every MPI communication API, including the collective operations, as the reference</p>
<footer id='303' style='font-size:18px'>38</footer>
<p id='304' data-category='paragraph' style='font-size:14px'>to an MPI process group, with the rank number to identify the individual process in the<br>communicator. The basic communicator MPI_COMM_ WORLD is provided by MPI by<br>default with every MPI process inside. The user can also freely create and manipulate<br>the communicator to create any arbitrary MPI process group for better communication<br>management, such as creating separate communicators for 'odd' and 'even' MPI process<br>number to only communicate among themselves.</p>
<p id='305' data-category='paragraph' style='font-size:14px'>In MPI implementations, such as MPICH, and its descendants such as Intel MPI, the<br>matching process for any communication is handled in a global matching queue, regardless<br>of the communicator, while Open MPI handles the matching with a local per-communicator<br>matching queue. The two approaches can inflict different performance impacts when it comes<br>to multi-threaded environments in MPI, as the threads race for access to the communicator<br>and the matching process. In the latter design, increasing the number of the communicator<br>might alleviate the stress on each communicator and provide a more level playing field<br>for a multi-threaded environment. Multirate provides the option for the user to switch<br>between utilizing a single communicator, or using an exclusive communicator for each of the<br>communication pairs, to provide a way to detect the communicator congestion problems of<br>the MPI implementations.</p>
<h1 id='306' style='font-size:20px'>3.6 Experimental Evaluation</h1>
<p id='307' data-category='paragraph' style='font-size:16px'>While Multirate offers bandwidth, message rate, and latency measurement, only the message<br>rate measurement is presented in this section, as it is the most representative metric for<br>the quality of multi-threaded communication. Nonetheless, the goal of the experiments is<br>to demonstrate the potential of Multirate in evaluating MPI performance and pinpointing<br>possible bottlenecks in current MPI implementations.</p>
<p id='308' data-category='paragraph' style='font-size:16px'>The experiment's results are from the University of Tennessee, Knoxville's Alembert<br>cluster. Each Alembert node consists of two sockets of Intel Xeon E5-2650v3 (Haswell) 10-<br>core CPUs, running at 2.3 GHz and configured with hyper-threading, with 64 GB of DDR4<br>2, and 133 MHz main memory. The interconnect is an InfiniBand (IB) EDR running at<br>100 Gbps. The MPI implementations in the experiments are: MPICH 3.3 Gropp (2002),</p>
<footer id='309' style='font-size:18px'>39</footer>
<p id='310' data-category='paragraph' style='font-size:16px'>Intel MPI 2018.1 IMPI, and Open MPI 4.0 Gabriel et al. (2004). MPICH and Open MPI are<br>configured with Unified Communication X (UCX) Shamis et al. (2015) library version 1.5.0<br>in multi-threaded support mode with all optimization turned on. The entire software stack,<br>including the MPI libraries, was compiled with GNU Compiler Collection (GCC) version<br>7.3.0 with maximum level of optimization flags from the provided package configure script,<br>while Intel MPI is only available in a pre-compiled binary from the vendor and assumed to<br>be operating with high efficiency on the platform.</p>
<br><p id='311' data-category='paragraph' style='font-size:14px'>The MPI process binding policy is bind to core (one core per MPI rank) for process mode<br>experiments, and every necessary precaution has been taken to ensure all experiments were<br>using identical bindings and thread placements. The thread mode experiments spawn only<br>one MPI process on each node with a "floating" (no binding/ bind to all available cores) policy<br>and manually bind the threads to their corresponding cores. The default communication<br>module of every MPI implementation is used, unless stated otherwise. The message sizes used<br>in these experiments were selected based on ongoing optimization efforts for MADNESS and<br>PaRSEC, by taking the sizes of the most representative communications. The performance<br>data points presented are the average of 30,000 runs, and, where meaningful, the standard<br>deviation is reported in the graph as error bars. Experiments that did not complete, either<br>due to a segfault in the MPI library or to a deadlock, identified by an expiring allocation<br>limit, were not reported and can be seen by the lack of data in the graphs.</p>
<p id='312' data-category='paragraph' style='font-size:16px'>To give a better understanding of the order of magnitude of the results, some graphs<br>report the theoretical upper limit of the message rate calculated by dividing the peak<br>hardware bandwidth of 100 Gbps by the corresponding message size. The computation<br>of the theoretical upper limit ignores all local overheads, and is, therefore, an unattainable<br>upper bound, toward which the message rate should asymptotically converge. The theoretical<br>upper bound on the message rate is 12.5 Mmsg/s for 1,024 bytes messages and 3.125 Mmsg/s<br>for 4,096 bytes messages (M stands for Millions).</p>
<p id='313' data-category='paragraph' style='font-size:20px'>3.6.1 Communication Patterns</p>
<p id='314' data-category='paragraph' style='font-size:16px'>This section investigates how different state-of-the-art MPI implementations handle different<br>communication patterns available in Multirate. Most figures in this section will illustrate</p>
<footer id='315' style='font-size:16px'>40</footer>
<p id='316' data-category='paragraph' style='font-size:14px'>the performance of both process and thread mode for the same communication pattern in<br>the same figure. However, the discussion is organized as follows: First, the performance<br>of the process mode in each communication pattern, which is a general use-case for most<br>applications. It also serves as the practical upper bound for the thread mode since the<br>threading performance with additional overhead is unlikely to achieve the better performance<br>than the process mode in the same settings. The discussion is then followed by the thread<br>mode performance and comparisons at the end of this section.</p>
<p id='317' data-category='paragraph' style='font-size:20px'>Pairwise</p>
<p id='318' data-category='paragraph' style='font-size:14px'>Pairwise communication performance is illustrated in Figures 3.5 and 3.6. For the<br>experiment, the message size and the window size (number of messages per iteration) is<br>fixed, while the number of communication pairs varies. Since the communication pattern is<br>pairwise, the number of sender and receiver entities is the same. Every communication pair<br>has the same amount of workload.</p>
<p id='319' data-category='paragraph' style='font-size:14px'>The solid lines (Figures 3.5 and 3.6) show the performance in process mode for three<br>MPI implementations. The message rate increases with the number of process pairs, as<br>anticipated. Since each individual process pair communicates independently without any<br>contention, the more pair added gives higher aggregated message rate, until the performance<br>comes close to the theoretical limit of the hardware device and the message rate flattens out.<br>This indicates that in this mode, the MPI implementations can operate very efficiently and<br>push the hardware to its limit. Each MPI implementation reacts slightly differently, but all<br>of them asymptotically reach the peak message rate at a number of communication pairs. In<br>this experiment, for 1,024-byte messages, Open MPI reaches the peak with 7 communication<br>pairs, while Intel MPI needs 11 pairs, and MPICH needs 16 at around 11M msg/s, with the<br>calculated bandwidth of 90.12 Gbps or 90% of the theoretical network bandwidth. Although<br>the MPI implementations reach the peak performance at different point, the result firmly<br>suggested that the MPI requires more than one communication pair to satisfy the peak<br>hardware bandwidth for small size messages.</p>
<p id='320' data-category='paragraph' style='font-size:16px'>At 4,096 bytes, the MPI implementations reach the peak message rate earlier (Figure 3.6),<br>as by increasing the message size, the actual transfer time increases and the software</p>
<footer id='321' style='font-size:16px'>41</footer>
<figure><img id='322' style='font-size:14px' alt="Pairwise 1024 bytes, window size = 128, Alembert
Theoretical Peak Message Flate
12 M
rate/s
8 M ompi_process
ompi_ threads
Message impi_process
impi_ threads
mpich _process
mpich_ threads
4 M
0 M
0 5 10 15 20
1.5 M
rate/s
Message 1 M
0.5 M
0 M
0 10 15 20
Number of communication pairs" data-coord="top-left:(276,185); bottom-right:(999,812)" /><figcaption id='324' style='font-size:16px'>Pairwise 4096 bytes, window size = 128, Alembert</figcaption></figure>
<p id='323' data-category='paragraph' style='font-size:20px'>Figure 3.5: Pairwise message rate for a message size of 1,024 bytes, W 128.</p>

<figure><img id='325' style='font-size:14px' alt="Theoretical Peek Message Rat⌀
3 M
2 M
rate/s
ompi_process
ompi_ threads
Message impi_process
impi_ threads
mpich_process
mpich_ threads
1 M
0 M
0 10 15 20
Number of communication pairs" data-coord="top-left:(282,959); bottom-right:(996,1415)" /><figcaption id='326' style='font-size:18px'>Figure 3.6: Pairwise message rate for a message size of 4,096 bytes, W 128.</figcaption></figure>

<footer id='327' style='font-size:22px'>42</footer>
<p id='328' data-category='paragraph' style='font-size:20px'>overheads have lesser impact on the overall performance. With larger message sizes, it<br>requires smaller numbers of concurrent messages to reach the peak hardware bandwidth<br>(Around 3M msg/s, the calculated bandwidth is 98.3 Gbps, 98.3% of the theoretical<br>bandwidth), and the differences between MPI implementations are mostly negligible.</p>
<p id='329' data-category='paragraph' style='font-size:14px'>The performance results of the process mode in pairwise pattern shows that with truly<br>concurrent communication, the MPI implementation can attain the peak performance of<br>the network device. We can use this process mode performance as the reference point,<br>as an attainable performance or 'practical peak' for multi-threaded communication on the<br>same hardware. It should be noted that threading communications perform extremely<br>suboptimally for every MPI implementation in this experiment, with a global message<br>rate decreasing as the number of peers increased opposite to what we observe from non-<br>threading communication despite running on the same set of hardware. The message<br>rate for thread mode never goes above 1 /12th of the practical peak and suggests massive<br>optimization opportunities for every MPI implementation.</p>
<h1 id='330' style='font-size:16px'>Many to One</h1>
<p id='331' data-category='paragraph' style='font-size:16px'>In this experiment, the number of receiver entities is fixed to one, while varying the number<br>of the sender entities to measure the limit of the message extraction capability of a single<br>receiver entity. Generally, the message rate should keep increasing with the number of<br>senders until it reaches the capacity of the receiver to handle the incoming messages, then<br>the message rate should stay flat from that point.</p>
<p id='332' data-category='paragraph' style='font-size:16px'>The solid lines on Figure 3.7 show the performance in process mode for 1,024-byte<br>messages. Intel MPI performs the best and shows a performance improvement with an<br>increasing number of senders. This result suggests that one receiver process is capable of<br>extracting more incoming messages than a single sender can inject. From the result, the<br>Multirate benchmark can expose the optimal point of operation for Intel MPI at 2.8M<br>msg/s with 8-12 senders. Open MPI shows steady performance at 1.5M msg/s despite the<br>increasing number of senders, indicating the limit of message extraction from the receiver.<br>The result indicates that there is more room for improvement in the message extraction<br>path of Open MPI. On the other hand, MPICH demonstrates a performance decrease with</p>
<footer id='333' style='font-size:20px'>43</footer>
<p id='334' data-category='paragraph' style='font-size:16px'>Many to one 1024 bytes, window size = 128, Alembert</p>
<figure><img id='335' style='font-size:14px' alt="2 M
rate/s
ompi_process
ompi_threads
Message impi_process
impi_threads
mpich_process
1 M mpich_threads
0 M
0 10 15 20
Number of senders" data-coord="top-left:(217,170); bottom-right:(1053,646)" /><figcaption id='336' style='font-size:18px'>Figure 3.7: Many-to-one message rate with a message size of 1,028 bytes, w = 128.</figcaption></figure>

<p id='337' data-category='paragraph' style='font-size:22px'>higher number of senders, which indicates the internal bottleneck that introduces the delays<br>proportionate to the volume of incoming messages, decreasing the rate of extraction, leading<br>to sub-optimal performance the behavior that is not anticipated.</p>
<p id='338' data-category='paragraph' style='font-size:22px'>For 4,096-byte messages (Figure 3.8, solid lines), Open MPI demonstrates better<br>performance early on but drops off with an increasing number of senders- entirely different<br>behavior from the smaller 1,024-byte messages. The behavior shows that the same MPI<br>implementation reacts differently depending on the message sizes. The change of behavior<br>can originate from the protocol change at the MPI implementation level or even from the<br>underlying network library (in this case, Open UCX). On the other hand, for both message<br>sizes, MPICH and Intel MPI show similar behaviors. Specifically, Intel MPI shows the same<br>pattern of a sharp rise in message rate when increasing the number of senders from 7 to 8.<br>This information can become valuable for the application developer in deciding their MPI<br>process layouts to get the most performance out of their MPI implementation. From an<br>MPI developer's perspective with inside knowledge of the code base, this information can be<br>useful for further optimization.</p>
<footer id='339' style='font-size:20px'>44</footer>
<p id='340' data-category='paragraph' style='font-size:16px'>Many to one 4096 bytes, window size = 128, Alembert</p>
<figure><img id='341' style='font-size:14px' alt="1.6 M
1.2 M
rate/s
ompi_process
ompi_threads
Message impi_process
0.8 M impi_threads
mpich_process
mpich_ threads
0.4 M
0 5 10 15 20
Number of senders" data-coord="top-left:(217,165); bottom-right:(1054,647)" /></figure>
<p id='342' data-category='paragraph' style='font-size:18px'>Figure 3.8: Many-to-one message rate with a message size of 4,096 bytes, w = 128.</p>
<p id='343' data-category='paragraph' style='font-size:20px'>The many-to-one experiments show that the Multirate benchmark can successfully<br>identify the problem in the message extraction path, and expose the optimal performance<br>point for a specific workload from the receiver's perspective.</p>
<p id='344' data-category='paragraph' style='font-size:22px'>One to Many</p>
<p id='345' data-category='paragraph' style='font-size:20px'>This experiment utilizes one sender to send messages to the increasing number of receivers.<br>In reverse of many-to-one, by having multiple receivers to extract the messages at the same<br>time, this communication pattern reduces the possibility of congestion on the receiver and<br>focuses on the capability of a single sender to inject the messages into the network. Generally,<br>with many receivers to absorb the communication, the message rate should keep increasing<br>until the sender reaches its peak injection rate and then flattens out beyond that point.</p>
<p id='346' data-category='paragraph' style='font-size:20px'>Figures 3.9 and 3.10 show the message rate of 1,024 and 4,096 byte messages when<br>increasing the number of receivers. This section focuses only on the performance of the<br>process mode (solid lines). For 1,024-byte messages, all three MPI implementations show<br>better performance with the increasing number of receivers until they reach a peak an<br>anticipated behavior. The result suggests that a single receiver cannot efficiently extract the</p>
<footer id='347' style='font-size:22px'>45</footer>
<p id='348' data-category='paragraph' style='font-size:18px'>One to 1024 window size 128, Alembert</p>
<figure><img id='349' style='font-size:14px' alt="many bytes, =
3 M
rate/s
2 M
ompi_process
Message threads
ompi_
impi_process
impi_ threads
mpich _process
1 M mpich_threads
0 M
0 10 15 20
Number of receivers" data-coord="top-left:(219,223); bottom-right:(1054,706)" /><figcaption id='350' style='font-size:20px'>Figure 3.9: The one-to-many message rate with a message size of 1,024 bytes, W 128.</figcaption></figure>

<figure><img id='351' style='font-size:16px' alt="One to many 4096 bytes, window size = 128, Alembert
2 M
1.5 M
rate/s
ompi_process
ompi_threads
impi_process
Message
impi_ threads
1 M mpich_process
mpich_threads
0.5 M
10 15 20
Number of receivers" data-coord="top-left:(218,897); bottom-right:(1059,1393)" /><figcaption id='352' style='font-size:20px'>Figure 3.10: The one-to-many message rate with a message size of 4,096 bytes, W 128.</figcaption></figure>

<footer id='353' style='font-size:20px'>46</footer>
<p id='354' data-category='paragraph' style='font-size:18px'>messages injected from a single sender, indicating that every MPI implementation in this<br>experiment exhibits some level of imbalance between the message injection and extraction.</p>
<p id='355' data-category='paragraph' style='font-size:14px'>Compared to other implementations, Open MPI gives the best performance in this<br>communication pattern. The best performance point of Open MPI for this setting is around<br>4-7 receivers at 3M msg/s, and the calculated bandwidth is 24.57 Gbps, only 1 / 4th of<br>the theoretical network bandwidth. Comparing this result to the pairwise communication<br>pattern, which can achieve up to 90% of the theoretical bandwidth, this experiment also<br>shows that a single sender cannot achieve the optimal injection rate to satisfy the peak<br>network bandwidth for 1,024 byte messages. The result showed an unexpected small<br>performance drop-off for all three MPI implementations after increasing the number of<br>receivers, a behavior that is not presented when increasing the message size to 4,096 bytes,<br>which has to be investigated further.</p>
<p id='356' data-category='paragraph' style='font-size:14px'>At the message size of 4,096 bytes, every MPI implementation can reach the plateau<br>around 5 receivers. The Intel MPI gives better performance early on with a lower number<br>of receivers. With larger messages, the message rate required to achieve the peak network<br>bandwidth becomes lower. For example, in this experiment, Open MPI's message rate<br>plateaued out around 2.5M msg/s. The calculated bandwidth for the message size of 4,096<br>bytes is 81.92 Gbps, or 81.92% of the theoretical network bandwidth of the hardware used.</p>
<p id='357' data-category='paragraph' style='font-size:14px'>Together, the experiment results of the one-to-many and many-to-one demonstrated that<br>(1) In most MPI implementations, the single receiver cannot extract the incoming messages<br>fast enough. The experiment shows that a single sender can satisfy at least 2 receivers in<br>some workloads. (2) Increasing number of concurrent messages from the senders' side might<br>not translate into higher message rate, as the capacity of the extraction process is limited<br>by the receivers.</p>
<footer id='358' style='font-size:18px'>47</footer>
<h1 id='359' style='font-size:20px'>Many to Many</h1>
<p id='360' data-category='paragraph' style='font-size:14px'>Moving towards a more dynamic relationship between the number of senders and receivers,<br>this experiment illustrates the message rate of the many-to-many communication pattern<br>(every sender to every receiver) from a different perspective. Figure 3.11 shows the<br>performance when increasing the number of senders against fixed sets of receivers, while<br>Figure 3.12 shows the opposite. This section only focuses on the process mode result (top<br>half of the figure), as further discussion on the threading performance is listed separately in<br>Section 3.6.3.</p>
<p id='361' data-category='paragraph' style='font-size:14px'>The experiment results illustrated the different behavior of each MPI implementation.<br>Figure 3.11 shows that Intel MPI can perform better with more than one receiver, but<br>increasing the receivers will not give much benefit, as it seems to already reach top<br>performance at around 5 receivers; while Open MPI and MPICH reach top performance<br>at some specific point, the message rate drops off with an increasing number of senders.<br>Unlike Intel MPI, increasing the number of receivers provides some performance impact<br>for both Open MPI and MPICH. From a different perspective, Figure 3.12 also shows that<br>Intel MPI is performing well regardless of the number of senders or receivers, while Open MPI<br>and Open MPI provide a similar result. The two figures show the most efficient performance<br>point of each MPI implementation. For example, Open MPI seems to run optimally around<br>20 receivers with 4-5 receivers for this particular workload. The result also illustrated the<br>similarity between Open MPI and MPICH, where both implementations utilize Open UCX<br>as their underlying network library.</p>
<br><p id='362' data-category='paragraph' style='font-size:14px'>This experiment presented the versatility of the Multirate benchmark and its ability to<br>expose the optimal performance point of each MPI implementation, which can be beneficial<br>to application developers to design their communication workload. The MPI developer can<br>also use this information to get the better understanding of their MPI implementation under<br>a specific workload for better optimization.</p>
<footer id='363' style='font-size:16px'>48</footer>
<figure><img id='364' style='font-size:14px' alt="Intel MPI 2018.1 Open MPI 4.0 MPICH 3.3
8M 8M 8M
Number of receivers
rate
Message 1
6 M 6M 6M
5
10
20
-
4 M 4M 4M
Process mode
2 M 2M 2M
0M 0M 0M
5 10 15 20 10 15 20 10 15 20
mode
2 M 2M 2M
M- 1M 1M
Thread
0 M 0M 0M
10 15 20 10 15 20 5 10 15 20
Number of senders" data-coord="top-left:(144,442); bottom-right:(1129,737)" /><figcaption id='365' style='font-size:16px'>Figure 3.11: The many-to-many communication performance with a fixed number of<br>receivers.</figcaption></figure>

<figure><img id='366' style='font-size:14px' alt="Intel MPI 2018.1 Open MPI 4.0 MPICH 3.3
8M 8M 8M
Number of receivers
5
10
Message rate
6M 6M 6M
20
-
4M 4M 4M-
mode
Process
2 M 2M 2M
0M 0M 0M
8 12 16 10 15 20 10 15 20
mode
2 M 2M 2M
Thread
1 M 1M 1M
0 M 0M 0M
12 16 10 15 10 15 20
Number of receivers" data-coord="top-left:(145,862); bottom-right:(1133,1155)" /><figcaption id='367' style='font-size:18px'>Figure 3.12: The many-to-many communication performance with a fixed number of senders.</figcaption></figure>

<footer id='368' style='font-size:20px'>49</footer>
<h1 id='369' style='font-size:20px'>3.6.2 Variable workload</h1>
<p id='370' data-category='paragraph' style='font-size:14px'>While the earlier experiments demonstrated the different behaviors of MPI implementations<br>with respect to different message sizes, this section performs the experiments on the different<br>communication patterns with different window sizes (number of messages per iteration) and<br>observes how MPI implementations react to different workloads.</p>
<br><p id='371' data-category='paragraph' style='font-size:14px'>For one-to-many communication (middle row), Intel MPI still shows similar scaling to<br>the many-to-one experiment with a small drop-off at the end. Open MPI performs well in<br>this communication pattern with a higher message rate overall for every window size, while<br>MPICH demonstrates good performance, but also with a small drop-off later on.</p>
<p id='372' data-category='paragraph' style='font-size:14px'>It has been learned from the earlier experiment that a pairwise communication pattern<br>gives the best performance and scaling with increasing number of pairs. For this<br>experiment, Figure 3.13 (bottom row) indicates that the window size also affected the overall<br>communication performance. Generally speaking, for this particular message size, a larger<br>window size allows more messages to be injected per iteration and increases the message rate<br>until it reaches the limit of the network device and plateaus out. The performance of each<br>MPI implementation is slightly different but still follows a general trend.</p>
<p id='373' data-category='paragraph' style='font-size:14px'>In this experiment, the Multirate benchmark shows that it can expose the behaviors of<br>the different MPI optimization. For example, it can be concluded that Intel MPI is very<br>sensitive to the window size and will perform well with a larger one. Open MPI is not as<br>sensitive, as the results show similar performance across all window sizes but in many-to-one,<br>the window size does not affect the performance at all. This behavior indicates that there<br>is some limitation in its message extraction capability. MPICH is struggling in many-to-one<br>communication, especially with higher numbers of messages, consistent with the findings<br>from the earlier experiment. MPICH developers can use this information to pinpoint the<br>origin of the performance bottleneck in their implementation.</p>
<p id='374' data-category='paragraph' style='font-size:14px'>While the result of this experiment is measured from a single message size in process<br>mode, it demonstrates that Multirate is capable of exposing the optimal points of each MPI<br>implementation. Users can simply tweak the parameters to perform measurement on their<br>desired workload and mode operation.</p>
<footer id='375' style='font-size:16px'>50</footer>
<figure><img id='376' style='font-size:14px' alt="Intel MPI 2018.1 Open MPI 4.0 MPICH 3.3
Window size
rate 3 M 3M 3M 16
Message
32
64
128
2 M 2M 2M
- 256
euo 이
1 M 1M 1 M
Many
0M 0M 0M
5 10 15 20 5 10 15 20 10 15 20
Number of senders
Rate
m M 3M 3M
Message
2 M 2M 2M
/ Window size
Many
16
32
to 1 M 1M 1M
64
One
128
256
0M 0M- 0M-
10 15 20 5 10 15 20 10 15 20
Number of Receivers
Rate 9 M 9M
9M
Message
6 M 6M- Window size
6M
/
16
Pairwise
32
64
3 M 3M 128
3M
256
10 15 20 10 15 20 10 15 20
Number of Pairs" data-coord="top-left:(141,480); bottom-right:(1133,1078)" /></figure>
<p id='377' data-category='paragraph' style='font-size:18px'>Figure 3.13: Message rate (1,024 bytes) on different communication patterns on multiple<br>window sizes.</p>
<footer id='378' style='font-size:18px'>51</footer>
<h1 id='379' style='font-size:20px'>Case study</h1>
<p id='380' data-category='paragraph' style='font-size:14px'>Other than using Multirate to compare the performance between MPI implementations, it<br>can also be used to expose the performance issues in a single implementation. The case<br>study is the comparison between the two stable releases of Open MPI, 4.0 and 3.0.</p>
<p id='381' data-category='paragraph' style='font-size:14px'>This experiment only uses one pair of communication entities and increases the window<br>size. Since the window size per communication pair in the early experiment is 128, the<br>window size is increased in multiples of 128 in 20 steps. Internally, Open MPI 4.0 uses a<br>pml / ucx network module as default, while Open MPI 3.0 uses btl/ openib. Though the two<br>modules have significant differences, both of them are designed to efficiently operate with<br>Mellanox's Infiniband hardware.</p>
<p id='382' data-category='paragraph' style='font-size:14px'>The result is demonstrated in Figure 3.14. For the default UCX module, the message<br>rate does not increase with the window size, thereby confirming the finding that a single<br>entity cannot satisfy the network bandwidth. For the btl/ openib module, this experiment<br>exposes a sub-optimal implementation of the network module, as the result shows the<br>performance drop when increasing the message size with a high variation between runs.<br>After some investigation, I find that btl/ openib has a poor credit management system,<br>which leads to starvation of network send credits under a heavy workload. The behavior<br>is non-deterministic, as the starvation and the recovery occur at different points for every<br>run. I indirectly adjust the number of send credits in btl/ openib module the via Modular<br>Component Architecture (MCA) Squyres parameters offered by Open MPI and can mitigate<br>the credit problem from the module (marked with * in the figure).</p>
<h1 id='383' style='font-size:22px'>3.6.3 Multithread MPI</h1>
<h1 id='384' style='font-size:18px'>Overhead of threading</h1>
<p id='385' data-category='paragraph' style='font-size:14px'>This experiment uses the pairwise process mode when initializing MPI with MPI_Init and<br>MPI_Init_thread with MPLTHREAD MULTIPLE, without spawning any other thread, to<br>show the minimum overhead from each MPI thread safety implementation. Figure 3.15<br>compares the performance between MPI implementations. We can see only a 3-5%<br>performance decrease from MPI implementations for this communication pattern. However,</p>
<footer id='386' style='font-size:16px'>52</footer>
<p id='387' data-category='paragraph' style='font-size:20px'>Pairwise 1024 bytes, 1 sender, 1 receiver, Alembert</p>
<figure><img id='388' style='font-size:14px' alt="Theoretical Peak Message Flate
12 M
Open MPI_4.0
Open MPI_3.0
Open MPI_3.0*
8 M
rate/s
Message
4 M
0 M
1000 2000
Window size" data-coord="top-left:(225,196); bottom-right:(1052,724)" /><figcaption id='389' style='font-size:22px'>Figure 3.14: Performance difference between two Open MPI release versions; 1,024 bytes<br>pairwise, process to process mode.</figcaption></figure>

<caption id='390' style='font-size:18px'>Pairwise 1024 bytes, window size = 128, Alembert</caption>
<figure><img id='391' style='font-size:16px' alt="12.5 M
Theoretical Peak Message Rate
10 M
rate/s
ompi_process
ompi_process mt
7.5 M
Message
수 impi_process
impi_process mt
mpich_process
mpich_process_ mt
5 M
2.5 M
10 15 20
Number of communication pairs" data-coord="top-left:(230,889); bottom-right:(1050,1435)" /><br><figcaption id='392' style='font-size:22px'>Figure 3.15: Minimum cost of thread safety.</figcaption></figure>

<footer id='393' style='font-size:22px'>53</footer>
<p id='394' data-category='paragraph' style='font-size:20px'>It has been shown in the earlier experiments that the performance will drop drastically<br>with the increasing number of threads (Figure 3.5), indicating that most of the performance<br>degradation originated from the thread contention.</p>
<h1 id='395' style='font-size:16px'>Different communication patterns</h1>
<p id='396' data-category='paragraph' style='font-size:16px'>The dashed lines in Figures 3.5 and 3.6 show the message rate of the increasing number<br>of communication pairs with a pairwise pattern in thread mode. For both message sizes,<br>Intel MPI and MPICH have similar performance, but the message rate of Open MPI drops<br>at first and then bounces back with more communication pairs. However, all of them suffer<br>significant performance loss compared to the results in process mode. The dashed lines in<br>Figures 3.7-3.10 show the performance of many-to-one and one-to-many patterns in thread<br>mode.</p>
<p id='397' data-category='paragraph' style='font-size:16px'>For many-to-one, every MPI implementation seems to suffer a performance loss when<br>introducing more sender threads. Open MPI shows a performance drop early on, but<br>performance recovers after increasing the number of senders, while Intel MPI and MPICH<br>performance gradually decreases.</p>
<br><p id='398' data-category='paragraph' style='font-size:16px'>In one-to-many communication, the performance drops with more communication pairs<br>for all three implementations. Open MPI is the best among the tested MPI implementations,<br>but none of them shows comparable performance with process mode.</p>
<br><p id='399' data-category='paragraph' style='font-size:14px'>many-to-many communication shows the significant difference between process mode<br>and thread mode. While process mode can handle this type of communication easily, thread<br>mode -in some cases cannot run to completion before the 30 second timeout.</p>
<br><p id='400' data-category='paragraph' style='font-size:16px'>Ideally, in thread mode, running on the same hardware with the same communication<br>pattern should provide comparable performance to process mode. However, the result<br>shows that MPI in thread mode is significantly slower than its process mode counterpart<br>indicating that the designs of current state-of-the-art MPI implementations are not well<br>optimized for threading.</p>
<p id='401' data-category='paragraph' style='font-size:16px'>To further analyze the threading performance, we have to look deeply into the design<br>of each MPI implementation to identify the bottleneck. Some of the MPI implementations<br>evaluated in this experiment make use of a global message matching queue. Using multiple</p>
<footer id='402' style='font-size:20px'>54</footer>
<p id='403' data-category='paragraph' style='font-size:14px'>threads in communication can increase the contention on the matching queue which has to<br>be accessed sequentially. Usually, MPI implementations make use of a mutual execution<br>(mutex) lock to serialize the access to critical parts of the communication. The cost of<br>securing a mutex lock increases with the number of threads. The threading result from<br>Figure 3.5 is isolated and shown in Figure 3.16, displaying performance degradation with<br>the increasing number of communication pairs and suggests the serialization bottlenecks.</p>
<h1 id='404' style='font-size:18px'>Communicator's effect</h1>
<p id='405' data-category='paragraph' style='font-size:14px'>For Open MPI, btl/ uct is a non-default communication module that does not use global<br>matching but separates the message matching by the communicator. In Figure 3.16,<br>the btl/ uct is manually selected to perform the experiments with single and multiple<br>communicators. The result demonstrated that using multiple communicators to allow<br>multiple threads to perform matching concurrently yields a better message rate. It also<br>suggests that matching can be one of the bottlenecks for MPI in thread mode, and an MPI<br>developer should try to reduce the serialization in the process.</p>
<p id='406' data-category='paragraph' style='font-size:14px'>While Multirate offers a communicator's effect test for every communication pattern,<br>only the experiment for pairwise communication is selected for this study, as the default<br>network module for every MPI implementation makes use of global matching and shows<br>similar performance.</p>
<footer id='407' style='font-size:18px'>55</footer>
<figure><img id='408' style='font-size:14px' alt="1.5 M
rate/s
ompi_ threads
1 M impi_ threads
Message
mpich_ threads
ompi_uct_ threads
ompi_uct_threads_comm
0.5 M
0 M
10 15 20
Number of communication pairs" data-coord="top-left:(223,504); bottom-right:(1054,1062)" /><br><figcaption id='409' style='font-size:20px'>Figure 3.16: Zoomed-in graph for pairwise message rate for thread mode with btl/ uct to<br>demonstrate the effect of the communicator.</figcaption></figure>

<footer id='410' style='font-size:16px'>56</footer>
<h1 id='411' style='font-size:20px'>3.7 Conclusion</h1>
<p id='412' data-category='paragraph' style='font-size:16px'>This chapter presented the Multirate benchmark, a novel multi-threaded MPI performance<br>measurement tool. The main contributions of this chapter concerning Multirate are: (1)<br>the benchmark offers multiple communication patterns that can be used in conjunction with<br>different modes of operations, allowing quick comparison in various settings between MPI in<br>threading and non-threading environment; and (2) Utilizing the benchmark for performance<br>assessments of current state-of-the-art MPI implementations.</p>
<p id='413' data-category='paragraph' style='font-size:14px'>The experiments show the potential of the Multirate benchmark for evaluating, quantify-<br>ing and understanding the performance of MPI implementations under realistic, application-<br>provided workloads. The Multirate benchmark can benefit PI developers as one of the<br>evaluation tools used to identify bottlenecks in their implementations, or as a regression<br>testing tool -and also to users when making the decision on what MPI implementation<br>has the potential of maximizing the performance of their application. Moreover, Multirate<br>can be used as an optimization tool allowing quick testing of different sets of configuration<br>parameters for the different implementation protocols, and assessing which set provide the<br>best overall performance on a specific architecture and/ or platform.</p>
<p id='414' data-category='paragraph' style='font-size:14px'>In this study, the Multirate benchmark significantly contributes to the discovery of multi-<br>threaded environment bottleneck for Open MPI. The bottlenecks, along with the proposed<br>solutions, are discussed and presented in Chapter 5. The Multirate benchmarks is hosted on<br>Github under a BSD 3-Clause 'New' or 'Revised' License, and will be soon make available 1<br>The user guide for the benchmark is presented with this dissertation in Appendix A.</p>
<p id='415' data-category='footnote' style='font-size:18px'>1https: / / github.com/ICLDisco/mmlhirate</p>
<footer id='416' style='font-size:16px'>57</footer>
<h1 id='417' style='font-size:18px'>Chapter 4</h1>
<h1 id='418' style='font-size:20px'>Advance Thread Synchronization</h1>
<h1 id='419' style='font-size:16px'>4.1 Overview</h1>
<p id='420' data-category='paragraph' style='font-size:14px'>This chapter describes the current state and limitations of MPI_Wait and MPI_Test and<br>all of its variances (such as MPI_ Waitall, MPI_Waitsome, and MPI_ Waitany) in the multi-<br>threaded environment from the standpoint of an MPI implementation. I introduce the<br>thread synchronization object design, along with its API to equip the MPI implementation<br>with the means to control and redirect threads for better optimization. I demonstrate the<br>potential of my design by implementing MPI_Wait* with the thread synchronization object<br>to reduce the lock contention of the MPI progress engine and show that my implementation<br>can achieve up to 7x performance in shared memory communication and up to 3x inter-node<br>communication.</p>
<p id='421' data-category='paragraph' style='font-size:14px'>I further discuss the efforts, ongoing collaboration, and preliminary results regarding<br>the utilization of my thread synchronization object design to achieve more from thread<br>parallelism when initializing MPI with MPI_THREAD MULTIPLE, including my proposal<br>of the MPI extension to bring the thread synchronization object up to the user level.</p>
<h1 id='422' style='font-size:16px'>4.2 Introduction</h1>
<p id='423' data-category='paragraph' style='font-size:14px'>One of the major roadblocks for MPI implementations to optimize for threading performance<br>is the lack of threading information from the user level. Currently, there is no standardized</p>
<footer id='424' style='font-size:14px'>58</footer>
<p id='425' data-category='paragraph' style='font-size:14px'>way for the user to notify MPI with information such as how many threads they are<br>expecting to be performing communication, or even a piece of simple information such<br>as the thread identification to let the MPI implementation know which thread is calling<br>the MPI routine. There are several studies focused on establishing the infrastructure for<br>sharing thread information (or 'interoperability') between threading frameworks such as<br>POSIX thread (pthread) Lewis and Berg (1998) or OpenMP Dagum and Menon (1998)<br>and MPI implementations, but the idea has not been carried out by the MPI forum. Such<br>information, if obtained, would a positive impact on the MPI implementation as they can<br>design better algorithms to navigate through threads and utilize them properly, along with<br>allocating proper resources for their uses.</p>
<p id='426' data-category='paragraph' style='font-size:14px'>Rather than relying on the threading framework to provide the information, several<br>studies suggested a new API for the MPI standard to let the user manually manage the<br>threads through MPI. One of the suggestions is to allow the user to create endpoints under<br>the MPI rank. With multiple endpoints within an MPI rank, the user can map the endpoints<br>with threads, allowing them to address a specific thread at the target rank to perform<br>thread-to-thread communication while also providing the crucial threading information to<br>the MPI implementation for better threading optimization Dinan et al. (2014). However,<br>the suggestion comes with difficulties, such as the problem with collective operations and<br>more. The MPI forum has not yet approved the endpoint proposal but the work is still in<br>discussion Mpi-Forum (2016).</p>
<p id='427' data-category='paragraph' style='font-size:14px'>To address the lack of interoperability problem without relying on the support from<br>the MPI forum, I propose the design of thread synchronization object, an abstraction layer<br>to provide the MPI implementation more control over the user-level threads and redirect<br>them for better utilization without requiring any change from the application level. In this<br>chapter, I demonstrate the great benefit of my design for multi-threaded MPI_ Wait operation<br>and further discuss additional possibilities in utilizing the thread synchronization object to<br>harness the full power of thread parallelism.</p>
<footer id='428' style='font-size:18px'>59</footer>
<h1 id='429' style='font-size:20px'>4.3 Progress Engine Serialization</h1>
<p id='430' data-category='paragraph' style='font-size:14px'>From a high-level perspective, the MPI progress engine is the component that ensures<br>communication progress, either by moving bytes across the hardware, ensuring the expected<br>message matching, or guaranteeing MPI's FIFO message order requirement. From<br>an implementation perspective, the progress engine is the central place where every<br>component in an MPI implementation registers its progressing routine such as polling for<br>incoming messages, processing pending outgoing messages, including messages for collective<br>operations, or reporting completion to the user level. The design is illustrated in Figure 2.3<br>from chapter 2.</p>
<p id='431' data-category='paragraph' style='font-size:14px'>As the MPI standard does not provide an API for explicitly progressing messaging, calls<br>into the MPI progress engine occur under the hood during calls to other MPI routines.<br>The decision to enter the progress engine or not on a given MPI function call is up<br>to the MPI implementation, with the exception of blocking routines such as MPI_Send,<br>MPI_Recv or MPI_Wait where message progression, at least related to the operation itself,<br>is mandatory. That being said, the main purpose of the progress engine is to give the MPI<br>implementation the opportunity to check for message completion events from the network<br>and to ensure timely progress on non-blocking communications. MPI usually reads entries<br>from the completion queues (CQs) for completion events on a particular network endpoint.<br>Completion events can be from both incoming and outgoing messages. In the case of outgoing<br>message completion, MPI marks the corresponding send request as completed and doing SO<br>might release the user from a blocking call such as MPI_Send.</p>
<p id='432' data-category='paragraph' style='font-size:14px'>In a multi-threaded scenario, the MPI implementation has to ensure thread safety. Since<br>the progress engine is a centralized part where many other components register the progress<br>of their operations, and it is not guaranteed that every registered component will be thread<br>safe, the MPI implementation often casts a wide blanket by creating a large critical section for<br>the entire progress engine. While the coarse-grain approach is proven sufficient in providing<br>thread safety to the user, the contention on lock often hinders the overall performance of<br>the progress engine, especially with an increasing number of concurrent threads trying to<br>gain access to that critical section. The major cost from the lock semantic usually increases</p>
<footer id='433' style='font-size:16px'>60</footer>
<p id='434' data-category='paragraph' style='font-size:16px'>with the contention on the lock itself. I perform an experiment to demonstrate the cost of<br>securing a lock when increasing the number of threads and presents the result in Figure 4.1.<br>The result suggested that the cost of a lock operation increases in polynomial order with the<br>number of threads, consistent with earlier studies on lock-less data structures such as Amer<br>et al. (2016) and Amer et al. (2015).</p>
<h1 id='435' style='font-size:22px'>4.4 Synchronization Object</h1>
<p id='436' data-category='paragraph' style='font-size:16px'>In this study, I propose a novel approach for managing multiple threads from inside the MPI<br>implementation with the synchronization object (sync object). Traditionally, when multiple<br>threads are waiting for the completion of MPI requests, they race against one another to<br>execute the progress engine, which is protected by a lock, as described earlier in this chapter.<br>The race creates lock contention and degrades overall performance from the progress engine,<br>while under-utilizing thread parallelism as the lock acts as a funnel for only a single thread<br>to pass through. The sync object provides the MPI implementation with a mechanism to<br>redirect threads for other tasks and a work-tracking capability to release them back to the</p>
<h1 id='437' style='font-size:20px'>Average time to acquire lock. (Haswell)</h1>
<figure><img id='438' style='font-size:14px' alt="0.08
0.06
(msec) 0.04
Time
0.02
0.00
0 5 10 15 20
Number of threads" data-coord="top-left:(228,930); bottom-right:(1050,1435)" /><figcaption id='439' style='font-size:18px'>Figure 4.1: Cost of lock acquiring on Intel Xeon E5-2650 v3 (Haswell)</figcaption></figure>

<footer id='440' style='font-size:18px'>61</footer>
<p id='441' data-category='paragraph' style='font-size:14px'>user as soon as their waited requests are completed, providing a better opportunity for thread<br>utilization.</p>
<p id='442' data-category='paragraph' style='font-size:14px'>A sync object is an object with a simple reference counter that allows MPI requests to<br>attach to it. For every request attached, the sync counter increases its reference counter;<br>once a request completes, the MPI implementation can notify the associated sync object and<br>decrease its reference counter. This process allows MPI to notify only the thread involved<br>in the operation without involving other threads.</p>
<h1 id='443' style='font-size:20px'>Synchronization Object API</h1>
<p id='444' data-category='paragraph' style='font-size:18px'>For Open MPI's internal use, I create the sync object API and utilize it to redesign the<br>Open MPI progress engine. The API provides 4 methods to interact with the sync object<br>(INIT, WAIT, SIGNAL, and UPDATE). The accurate C API is located in the appendix of<br>this dissertation.</p>
<p id='445' data-category='list' style='font-size:16px'>· SYNC _INIT: Initialize the synchronization object.</p>
<p id='446' data-category='list' style='font-size:14px'>· SYNC_ WAIT: Blocking call, wait until signaled or the counter becomes zero.</p>
<p id='447' data-category='list' style='font-size:16px'>· SYNC _SIGNAL: Release the synchronization object from waiting.</p>
<p id='448' data-category='list' style='font-size:16px'>· SYNC_UPDATE: Add /subtract the number from the object's counter</p>
<h1 id='449' style='font-size:20px'>Implementing Wait Operation</h1>
<p id='450' data-category='paragraph' style='font-size:14px'>In asynchronous (non-blocking) communication, MPI returns an MPI request as a handle for<br>the user to track the status of the operation later with MPI_ Wait or MPI_Test. Generally,<br>we can categorize the request into two groups: send requests and receive requests. As their<br>name suggests, the send request is the request that is associated with a send operation and<br>the receive request is associated with a receive operation. Usually, an MPI request is marked<br>as completed when the MPI implementation receives the completion event from the network<br>by reading its completion queue (generally, through the progress engine). However, for the<br>receive requests, they can also be completed at posting; the message can arrive from the</p>
<footer id='451' style='font-size:18px'>62</footer>
<p id='452' data-category='paragraph' style='font-size:14px'>network before the user posts a corresponding receive for it, and the MPI implementation<br>matches the message with the request as soon as it is posted.</p>
<p id='453' data-category='paragraph' style='font-size:16px'>Traditional MPI_Wait* (waitall, waitsome, waitany) implementation involves a loop over<br>every request given at the user level, constantly checking for their completion, and simply<br>counting the number of completed requests in the loop. Once the number of completed<br>requests satisfies the wait condition (variants such as all, some, or any), the wait operation<br>is successful and returns to the user from the blocking call. If the condition is not satisfied, the<br>wait routine usually executes the progress engine to look for completion. In a multi-threaded<br>scenario, access to the progress engine is protected by a coarse-grain lock (Figure 4.2a and<br>Algorithm 1). As discussed earlier in this chapter, this creates a bottleneck and increases<br>the overall operation cost with the number of threads.</p>
<p id='454' data-category='paragraph' style='font-size:16px'>With the synchronization object API as a management layer, the MPI implementation<br>can become more efficient in redirecting threads for other purposes, and return it to the<br>user as soon as it needs to be returned (Figure 4.2b). I present the algorithm of the new<br>MPI_Wait* in Algorithm 2. In this implementation, MPI_ Wait* relinquishes the authority<br>of waiting to the synchronization object API, which can redirect the threads for other tasks.<br>The MPI_ Wait* will get notified from the synchronization object API (by returning from<br>SYNC_WAIT) when the waited requests are complete.</p>
<p id='455' data-category='paragraph' style='font-size:14px'>SYNC_WAIT can redirect the threads to different tasks. First, my implementation aims<br>to reduce the stress on the progress engine lock. I built a queue system which allows only a<br>single thread to execute the progress engine while the other threads wait peacefully, yielding<br>the usage of CPU core back to the user. Since all but one thread is yielding and not actively<br>trying to secure the lock, the contention on the progress engine lock is minimal, allowing<br>a single thread full access to the progress engine. The executing thread is referred to as a<br>'progress owner'.</p>
<p id='456' data-category='paragraph' style='font-size:16px'>When executing the progress engine, the progress owner can complete any pending<br>request from any thread. Once a request completes, the sync object associated with it gets<br>updated directly via SYNC_UPDATE. If a sync object's counter reaches zero, the progress<br>owner issues a signal via SYNC_SIGNAL to the corresponding sync. Since a sync object is<br>directly associated with a thread performing wait, when signaled, the thread stops yielding</p>
<footer id='457' style='font-size:20px'>63</footer>
<figure><img id='458' style='font-size:14px' alt="WAIT WAIT WAIT
CHECK CHECK CHECK
LOCK
Progress
Engine
UNLOCK" data-coord="top-left:(166,261); bottom-right:(564,629)" /></figure>
<br><figure><img id='459' style='font-size:14px' alt="WAIT WAIT WAIT
SYNC_WAIT SYNC_WAIT SYNC_ WAIT
Sync Object Layer
Thread Management
Progress Other
Engine tasks" data-coord="top-left:(733,261); bottom-right:(1050,623)" /></figure>
<p id='460' data-category='paragraph' style='font-size:22px'>(a) Original</p>
<br><p id='461' data-category='paragraph' style='font-size:16px'>(b) Proposed with sync object</p>

<table id='463' style='font-size:16px'><tr><td colspan="2">Algorithm 1 Original MPL Waitall implementation</td></tr><tr><td>1:</td><td>function MPI_ WAITALL (n,requests)</td></tr><tr><td>2:</td><td>while true do</td></tr><tr><td>3:</td><td>c = 0</td></tr><tr><td>4:</td><td>for each requests do</td></tr><tr><td>5:</td><td>if request is complete then</td></tr><tr><td>6:</td><td>c ← c + 1</td></tr><tr><td>7:</td><td>if c is equal n then</td></tr><tr><td>8:</td><td>break;</td></tr><tr><td>9:</td><td>lock Progress Engine Lock</td></tr><tr><td>10:</td><td>call Progress Engine</td></tr><tr><td>11:</td><td>unlock Progress Engine Lock</td></tr><tr><td>12:</td><td>return</td></tr><br><caption id='462' style='font-size:20px'>Figure 4.2: MPI_ Wait* operation implementation in multi-threaded scenario.</caption></table>
<footer id='464' style='font-size:18px'>64</footer>

<br><table id='466' style='font-size:14px'><tr><td>1:</td><td>function MPI_WAITALL (n,requests)</td></tr><tr><td>2:</td><td>call SYNC_INIT (sync)</td></tr><tr><td>3:</td><td>c = 0</td></tr><tr><td>4:</td><td>for each requests do</td></tr><tr><td>5:</td><td>if request is complete then</td></tr><tr><td>6:</td><td>c ← c + 1</td></tr><tr><td>7:</td><td>else</td></tr><tr><td>8:</td><td>attach request to sync</td></tr><tr><td>9:</td><td>call SYNC_UPDATE (sync,c)</td></tr><tr><td>10:</td><td>call SYNC_WAIT (sync)</td></tr><tr><td>11:</td><td>return</td></tr><caption id='465' style='font-size:20px'>Algorithm 2 New MPI_Waitall implementation with synchronization object API.</caption></table>
<footer id='467' style='font-size:16px'>65</footer>
<p id='468' data-category='paragraph' style='font-size:14px'>and reschedules itself for execution, removing itself from the queue. In the case where the<br>progress owner's sync object counter becomes zero, it passes on the progress ownership to<br>the next sync object in the queue to take its place. This design guarantees that if there are<br>multiple threads calling MPI_ Wait, there will always be one thread executing the progress<br>engine.</p>
<h1 id='469' style='font-size:20px'>4.5 Experimental Evaluation</h1>
<p id='470' data-category='paragraph' style='font-size:14px'>For evaluation of my design, I measure the message rate by using the Ohio State University<br>(OSU) microbenchmark OSU and the Multirate benchmark on the University of Tennessee's<br>Alembert cluster in both shared-memory and inter-node communication via a high-speed<br>InfiniBand network. The performance result is illustrated in Figure 4.3a and the speedup in<br>Figure 4.3b.</p>
<p id='471' data-category='paragraph' style='font-size:14px'>For shared-memory intra-node communication where the communication is expected to<br>be very fast through a simple memory copy operation, we can see a significant speedup<br>from the original design, especially with a higher number of threads. The synchronization<br>object design greatly reduces the lock contention on the progress engine and we can see<br>up to 8x performance improvement. On the other hand, when the communication is<br>inter node via InfiniBand, high-performance network hardware, the performance gain is<br>up to 2.5x and slightly drops off after increasing the number of threads. Although the<br>overall performance is increasing, we can still see that using a single thread to perform<br>communication yields a better result than multiple threads. From my design, using a single<br>thread to execute the progress engine, the performance should at least flatten out around a<br>single thread performance. This result suggests other bottlenecks in the multi-threaded MPI<br>implementation. Further in this study, I identified and addressed the discovered bottlenecks.<br>The details are discussed thoroughly in chapter 5.</p>
<p id='472' data-category='paragraph' style='font-size:14px'>In the case of thread over-subscription (binding multiple threads to the same physical<br>CPU core), the original design suffers from multiple unnecessary context switches as the<br>threads blindly race to take the control of the progress engine, then perform the check on<br>each MPI request associated with it. The synchronization object design, with a proper</p>
<footer id='473' style='font-size:16px'>66</footer>
<figure><img id='474' style='font-size:14px' alt="Message Rate - 1024 bytes Speed up from synchronization object design
1.25 M
1M
750 mode
shared-memory
Rate
0.75 M
(%) inter-node
Message
Speedup
500
0.5 M
0.25 M 250
2 3 4 5 8 9 10
Number of threads 0
shared-memory inter-node Original-- Sync 2 3 5 6 8 9 10
Number of threads
(a) (b)" data-coord="top-left:(155,140); bottom-right:(1120,484)" /></figure>
<p id='475' data-category='paragraph' style='font-size:18px'>Figure 4.3: Performance gain from utilizing thread synchronization object in MPI_ Wait<br>implementation.</p>
<p id='476' data-category='paragraph' style='font-size:14px'>notification system, only performs context switching when it is necessary. Figure 4.4 shows<br>that with the thread synchronization object, the design can minimize the context switching<br>and achieve up to 250x performance for shared-memory communication and 60x for inter-<br>node communication. While the performance gain is massive, it is unlikely that the modern<br>HPC application is designed to operate in an over-subscription environment.</p>
<h1 id='477' style='font-size:22px'>4.6 Ongoing Research</h1>
<p id='478' data-category='paragraph' style='font-size:14px'>So far, the current usage of the synchronization object is only for serializing the progress<br>engine execution and reducing the lock contention to the progress engine. Despite<br>contributing to better threading performance in most cases, only the thread with progress<br>ownership gets to work while the others are yielding and get de-scheduled. There is potential<br>for more thread parallelism with the synchronization object design. This section explores<br>some of the potential use cases for thread parallelism from synchronization object with small<br>prototypes and proofs of concept.</p>
<p id='479' data-category='paragraph' style='font-size:20px'>4.6.1 User-Level Extension</p>
<p id='480' data-category='paragraph' style='font-size:14px'>The drawback of the synchronization object design is the cost of attaching and detaching<br>the request to the sync object. Since the scenario is multi-threaded, the attach and detach</p>
<footer id='481' style='font-size:16px'>67</footer>
<p id='482' data-category='paragraph' style='font-size:16px'>Message Rate (1024 bytes) - oversubscribed on a single core (Haswel</p>
<figure><img id='483' style='font-size:14px' alt="Original
1 M - With sync
shared-memory
inter-node
(s/bsw)
rate
0. 1 M
Message
0.01 M
2 6 8 10
Number of threads" data-coord="top-left:(232,514); bottom-right:(1055,1113)" /><br><figcaption id='484' style='font-size:20px'>Figure 4.4: Message Rate in thread over-subscription scenario.</figcaption></figure>

<footer id='485' style='font-size:20px'>68</footer>
<p id='486' data-category='paragraph' style='font-size:16px'>cannot be a simple assignment operation as it is prone to the race conditions (in the case<br>where the request becomes complete while attaching). The solution is an atomic compare<br>and swap operation, which is significantly more costly than a simple assignment operation.<br>The performance impact from the atomic operations is not severe for MPL Waitall, as every<br>request has to be completed before returning, limiting the chance of unnecessary detaching.<br>However, for MPI_ Waitany and MPI_ Waitsome, when the user usually calls with the same<br>set of requests over and over, the performance might not be optimal, as the requests have<br>to be attached and detached before returning them to the user for every call. The impact<br>prevents this design from becoming feasible for MPI_Test operations where the completion<br>is not required.</p>
<p id='487' data-category='paragraph' style='font-size:16px'>This section presents the extension to the MPI API to allow user-level usage of the<br>synchronization object. With the user-level API, the synchronization object can further<br>provide more flexibility and functionality for a multi-threaded MPI environment, including<br>avoiding unnecessary detach operations. I propose an extension of the synchronization object<br>to the user level through the MPIX notation to demonstrate the potential of my design.</p>
<h1 id='488' style='font-size:20px'>The MPIX Sync API</h1>
<p id='489' data-category='paragraph' style='font-size:18px'>I propose a new MPI object, MPIX _Sync, with 5 user-level APIs to interact with the<br>synchronization object: INIT, ATTACH, DETACH, QUERY and QUERY _BULK.</p>
<p id='490' data-category='list' style='font-size:16px'>· MPIX_Sync_init: Initialize the sync object.</p>
<p id='491' data-category='list' style='font-size:16px'>· MPIX_Sync_attach: Attach a request to the synchronization object with associated<br>callback data. The callback data will be returned as the reference to the user when the<br>request is complete in the query API. The request is detached from the sync object<br>automatically after its completion.</p>
<p id='492' data-category='list' style='font-size:14px'>· MPIX_Sync_detach: Detach a request from the sync object.</p>
<p id='493' data-category='list' style='font-size:16px'>· MPIX_Sync_query: Query the sync object for a request completion. Return the<br>callback data of a completed request. Similar to MPI_Testany API.</p>
<footer id='494' style='font-size:18px'>69</footer>
<p id='495' data-category='paragraph' style='font-size:20px'>· MPIX_Sync_query _bulk: Query the sync object for multiple request completions.<br>Return the callback data of completed requests. Similar to MPI_Testsome API.</p>
<p id='496' data-category='paragraph' style='font-size:18px'>For the implementation of the proposed API, I take the current design of the<br>synchronization object and expand its functionality to be appropriate to use from user level.<br>Each sync object consists of a completion counter and a completion queue to store the<br>callback data. Once the user attaches the request to the sync object with user-specified<br>callback data, the user relinquishes the request to the MPI implementation, and should now<br>only rely on the callback data they associated with the request. Figure 4.5 depicts the general<br>design of the API. The accurate C API, along with the user guide for the MPI extension,<br>can be found in the Appendix B.</p>
<p id='497' data-category='paragraph' style='font-size:18px'>When an operation completes, the callback data associated with the operation is added<br>to the completion queue, and the counter gets updated accordingly. The synchronization<br>object keeps track of the number of outstanding completions and the callback data for each<br>completion of the operations. The user can query the completion through QUERY, which<br>will return the callback data from the completion queue in first-come, first-serve manner.<br>This queue is protected for thread safety.</p>
<br><p id='498' data-category='paragraph' style='font-size:16px'>The MPI standard prohibits concurrent wait or test operations on the same MPI request.<br>For example, the user cannot perform MPI_Test on the same MPI request simultaneously<br>from multiple threads. However, it is a common practice for some categories of application<br>such as the runtime scheduler or the work-stealing programming model, which relies on</p>
<figure><img id='499' style='font-size:14px' alt="request
MPIX_Sync
Attach
cbdata
- refcounts
- completion queue
- mutex
Query cbdata
NULL" data-coord="top-left:(324,1140); bottom-right:(939,1408)" /><figcaption id='500' style='font-size:22px'>Figure 4.5: The MPIX _Sync API design.</figcaption></figure>

<footer id='501' style='font-size:20px'>70</footer>
<p id='502' data-category='paragraph' style='font-size:14px'>posting persistent wildcard receive requests in anticipation of a message or "task" from other<br>peers, and constantly performs MPI_Test on the requests to detect the incoming message.<br>The applications that make communication decisions at the runtime such as PaRSEC Bosilca<br>et al. (2013), rootsim Pellegrini et al. (2011) and Graph500 Ang et al. (2010) only use a<br>single thread for communication due to this limitation. The MPIX_Sync API, with proper<br>thread protection, allows for multiple threads to check for completion on the same set of<br>requests simultaneously through QUERY and QUERY BULK, providing the opportunity for<br>more flexible message completion routines with thread parallelism, increasing the usability<br>of MPI_THREAD _MULTIPLE.</p>
<p id='503' data-category='paragraph' style='font-size:14px'>As the synchronization object's counter always keep tracks of the number of outstanding<br>completions, it eliminates the need for the loop over every request to check for completion<br>status. The user can check the number of completions just by reading the value of the<br>counter, complexity: O(1) instead of O(n). Additionally, with user-level control, the user<br>can query the same synchronization object again for more completion of attached requests<br>without having to reattach them, circumventing the cost of atomic operation associated with<br>attaching/ detaching procedure.</p>
<p id='504' data-category='paragraph' style='font-size:14px'>With the user-defined callback data, my proposed API relieves the burden of bookkeeping<br>from the applications, as they no longer need to keep track of MPI requests. The user can<br>define their own completion scheme, or use the callback data in the same manner as the<br>"Active Message" approach to direct the flow of their application. The current API is still<br>evolving. I plan to explore the possibility of a user-level callback function where the MPI<br>will execute the user-provided function as soon as the request is completed.</p>
<br><p id='505' data-category='paragraph' style='font-size:14px'>For the evaluation, first I demonstrate the improvement from the proposed API by<br>timing each call of MPIX_Sync_query comparing to MPI_Testsome by varying the number<br>of requests given to the API. The result is illustrated in Figure 4.6. The MPIX_Sync API<br>has the advantage of using the counter to check for completion instead of looping over every<br>request. In the case of no completion, the MPIX_Sync_query gives optimal performance.<br>However, with a higher number of completed requests, the benefit starts to drop off and<br>becomes comparable to the original MPI_Testsome.</p>
<footer id='506' style='font-size:18px'>71</footer>
<p id='507' data-category='paragraph' style='font-size:16px'>For real-world application evaluation, as a collaboration with Reazul Hoque, a graduate<br>student from the University of Tennessee, we take PaRSEC Bosilca et al. (2013), a task-<br>based runtime, and modified its communication engine to use MPIX_Sync API instead of the<br>original MPI_Testsome for request completion. PaRSEC relies heavily on persistent requests<br>and only uses a single dedicated communication thread in MPI non-threading mode. We<br>perform the experiment on two different PaRSEC subroutines and demonstrate the result in<br>Table 4.1. First, ping-pong, which involves only the communication workload. We can see<br>the performance improvement of 13% for the small message, and the performance benefit<br>diminished as the message size increases. This is expected behavior as the larger the message,<br>the more execution time that will be spent in the actual communication thus, less impact<br>from MPI overhead. Second, we tested with PaRSEC DPLASMA, linear algebra operation.<br>We cannot observe a significant difference between the two APIs. However, it should be noted<br>that the PaRSEC communication engine is already highly optimized by utilizing techniques<br>such as re-packing the MPI requests to match the completion order. Thus, the performance<br>benefit from this design is expected to be minimal. At this stage, we have not yet altered<br>the PaRSEC communication engine to allow multiple threads for communication.</p>
<p id='508' data-category='paragraph' style='font-size:18px'>MPIX_Sync_query VS MPI_ Testsome</p>
<figure><img id='509' style='font-size:14px' alt="75 Requests type
none completed
all completed
(percent)
50
Speedup
25
0 50 100 150 200 250
Number of requests per call" data-coord="top-left:(280,919); bottom-right:(995,1427)" /><figcaption id='510' style='font-size:20px'>Figure 4.6: MPIX Sync_query performance comparing to MPI_Testsome.</figcaption></figure>

<footer id='511' style='font-size:18px'>72</footer>

<br><table id='513' style='font-size:18px'><tr><td>Pingpong Message Size (Bytes)</td><td>Speedup %)</td></tr><tr><td>400</td><td>13.73</td></tr><tr><td>4000</td><td>7.15</td></tr><tr><td>40000</td><td>3.1</td></tr><tr><td>400000</td><td>2.55</td></tr></table>
<br><table id='514' style='font-size:14px'><tr><td>Kernel</td><td>Speedup ( %</td></tr><tr><td>dpotrf</td><td>~0*</td></tr><tr><td>dgeqrf</td><td>~0*</td></tr><caption id='512' style='font-size:18px'>Table 4.1: PaRSEC performance speedup from MPIX_ Sync API.</caption></table>
<footer id='515' style='font-size:18px'>73</footer>
<h1 id='516' style='font-size:20px'>4.6.2 Thread Pool</h1>
<p id='517' data-category='paragraph' style='font-size:16px'>Yielding the CPU core back to the user might be great in the case of thread over-subscription,<br>as it reduces the chance of context switching between threads. However, with the current<br>hardware trend being more cores per chip, it is very likely that each thread will have a<br>one-to-one mapping to the physical CPU core. With this in mind, it is better to utilize the<br>CPU core while waiting instead of having them in the idling state.</p>
<p id='518' data-category='paragraph' style='font-size:16px'>The thread pool design enables the utilization of waiting threads. Instead of de-scheduling<br>the threads, the threads are constantly looking for tasks to execute. I implemented a task-<br>stealing model for the waiting threads. The task can be generated from any component of<br>the MPI implementation, including the progress engine. For example, the matching process<br>for a message can be passed off as a task. Generating tasks for other threads to execute<br>might shave off execution time of the critical path and gives a better overall performance<br>(Figure 4.7).</p>
<p id='519' data-category='paragraph' style='font-size:16px'>For demonstration, in an ongoing collaboration with Yicheng Li, at the University of<br>Tennessee, Knoxville, on his research of Open MPI datatype engine optimization, we utilize<br>the thread pool, task-stealing design to parallelize the packing operation of MPI vector<br>datatype messages. In this experiment, the packing operation (via MPI_Pack) is split into<br>several tasks while several threads are actively waiting to execute tasks in SYNC_WAIT.<br>Once the tasks are created and added to the queue, the waiting threads pick them up and<br>execute them in parallel. Figure 4.8 illustrates the achieved bandwidth with parallel packing<br>via thread pool design. We observe the speedup when the buffer size is beyond 100KB and<br>see the most benefit when the buffer size is around 10 MB. This is proof of concept that the<br>thread pool design is one of the approaches that can extract more thread parallelism from<br>threads performing the wait operation. Currently, we are experimenting with different task<br>types from inside the MPI implementation.</p>
<p id='520' data-category='paragraph' style='font-size:14px'>From the early evaluation, MPIX_Sync API is the worst case, performing on the same<br>level as MPI_Test API but provides more benefits in some cases. We have not yet evaluated<br>the performance impact in a multi-threaded environment.</p>
<footer id='521' style='font-size:18px'>74</footer>
<figure><img id='522' style='font-size:16px' alt="Thread pool Progress Owner
Sync 2 Sync 1 Sync 0
Execute
Generate Progress
Task queue
Engine
Other
components" data-coord="top-left:(352,299); bottom-right:(880,567)" /><figcaption id='523' style='font-size:22px'>Figure 4.7: Thread pool design utilizing the synchronization object.</figcaption></figure>

<figure><img id='524' style='font-size:14px' alt="Bandwidth from MPI Pack using thread pool
60
Number of threads
1
4
8
16
(GB/S)
32
40
40
Bandwidth
20
0
1e+04 1e+06 1e+08
Buffer Size (bytes)" data-coord="top-left:(275,848); bottom-right:(995,1373)" /><br><figcaption id='525' style='font-size:20px'>Figure 4.8: MPI_Pack performance when utilizing threads in the threadpool design.</figcaption></figure>

<footer id='526' style='font-size:18px'>75</footer>
<h1 id='527' style='font-size:18px'>4.6.3 Multi- Threaded Progress Engine</h1>
<p id='528' data-category='paragraph' style='font-size:14px'>The progress engine is a crucial part of MPI communication that still remains serial. Another<br>approach in utilizing thread parallelism is to execute the progress engine in parallel. The<br>synchronization object design as a thread management layer can be modified to allow<br>concurrent access to the progress engine, while still maintaining the capability of the thread<br>pool design and user-level design, increasing the opportunity for more thread optimization.<br>That being said, in order to attain the parallel progress engine, each component registering<br>itself to the progress engine has to become thread-safe, which imposes a burden onto the<br>component owner. Nonetheless, in this study, I focus mainly on this approach -to investigate<br>the potential of concurrent execution of the progress engine for true thread parallelism. I<br>discuss my design and implementation in detail in chapter 5.</p>
<h1 id='529' style='font-size:20px'>4.7 Conclusion</h1>
<p id='530' data-category='paragraph' style='font-size:14px'>This chapter introduces the thread synchronization object, a novel approach which provides<br>more nuance in thread management for MPI implementation. I utilize the synchronization<br>object to mitigate the known bottleneck at the MPI progress engine, allowing for better<br>performance for both normal and thread over-subscribed cases in MPI_Wait variants. I<br>showed the performance gain of 7x for shared memory and 3x for inter-node threading<br>communication.</p>
<p id='531' data-category='paragraph' style='font-size:14px'>I explore other potentials of the thread synchronization object to further harness the<br>power of thread parallelism in MPI. I presented several prototypes and proofs of concept<br>for my designs, including the extension of the concept to the user level, which will provide<br>more flexibility for MPI programming paradigm with better threading support. While the<br>research of these possibilities is ongoing, I showed significant benefits of the synchronization<br>object. Moving forward, I focus mainly on utilizing multiple threads inside the MPI progress<br>engine to speed up overall communication. The topic is discussed in detail in chapter 5.</p>
<footer id='532' style='font-size:16px'>76</footer>
<h1 id='533' style='font-size:20px'>Chapter 5</h1>
<h1 id='534' style='font-size:20px'>Design of True Thread Concurrency<br>in MPI</h1>
<h1 id='535' style='font-size:16px'>5.1 Overview</h1>
<p id='536' data-category='paragraph' style='font-size:14px'>In this chapter, I propose a design that enables true thread concurrency for the MPI<br>implementation. My design addresses two problems: (1) the resource contention when<br>multiple threads are accessing the same network resources to perform communication;<br>and (2) that only a single thread is allowed to execute the progress engine at a time,<br>ultimately eliminating the opportunity to utilize thread parallelism in communication. The<br>two intertwining problems are the remnants from the original bulk synchronization design<br>where only a single resource is available while multiple threads race to access it, creating<br>massive lock contentions.</p>
<p id='537' data-category='paragraph' style='font-size:14px'>I introduce the concept of Communication Resource Instances (CRIs), objects that<br>encompass every required resource to perform the communication, which can be allocated<br>multiple times. Next, I expand on the thread synchronization object from my work in<br>chapter 4 to propose a design that allows multiple threads to access the resource instances<br>in parallel, enabling them to perform multiple communication operations simultaneously.<br>I discuss the design in detail along with its benefits and shortcomings. I evaluate my<br>design with Multirate benchmark (chapter 3, Patinyasakdikul et al. (2019)) while obtaining<br>the internal information from Open MPI via the built-in software counters Eberius et al.</p>
<footer id='538' style='font-size:14px'>77</footer>
<p id='539' data-category='paragraph' style='font-size:16px'>(2017) for two-sided communication, and multi-threaded one-sided communication, RMA-<br>MT Dosanjh et al. (2016) benchmark.</p>
<br><p id='540' data-category='paragraph' style='font-size:14px'>The evaluation results show that my approach can achieve up to 2x performance gain<br>from the CRI design alone. Furthermore, I show that the parallel matching process is the key<br>to achieve better performance (up to 10x ) for multi-threaded MPI. Lastly, the results of my<br>design in one-sided multi-threaded communication illustrated that, without the matching<br>process, it can achieve up to 200x the performance of the original design.</p>
<h1 id='541' style='font-size:20px'>5.2 Background</h1>
<p id='542' data-category='paragraph' style='font-size:14px'>There are multiple challenges that need to be addressed in order to improve the performance<br>of the multi-threaded environment in MPI. This section presents a high-level background of<br>the communication process and the challenges in multi-threaded optimization from the MPI<br>implementation's perspective.</p>
<h1 id='543' style='font-size:18px'>Communication Resources</h1>
<p id='544' data-category='paragraph' style='font-size:14px'>In order to perform communication, the MPI implementations utilize the low-level network-<br>ing library such as socket, or verbs to interact with the network hardware. Recently, there<br>are efforts to unify the network library under a single standard such as the Open Fabric<br>Interface (OFI) Grun et al. (2015) or Open UCX Shamis et al. (2015) which provide the<br>high-level abstraction for the high-performance network devices with HPC capabilities such<br>as RMA and hardware tag-matching.</p>
<p id='545' data-category='paragraph' style='font-size:14px'>Generally, the MPI implementation interacts with the network hardware through the<br>allocated network resources with the associated network library. The resources such as<br>'network endpoint' are the handle to the network hardware. In order to perform the<br>communication, the MPI implementation has to issue the send or receive command to the<br>underlying network library with an endpoint. The other critical resource is the completion<br>queue (CQ) which is usually attached to an endpoint. When an operation completes, the<br>network library generates a completion event and put it in the completion queue. The MPI</p>
<footer id='546' style='font-size:16px'>78</footer>
<p id='547' data-category='paragraph' style='font-size:16px'>implementation has to read the completion queue and report the completion back to the<br>user-level appropriately.</p>
<p id='548' data-category='paragraph' style='font-size:14px'>Since the MPI implementation has to associate the completion event with the issued<br>operation from the user level, it has to store the information to look up later. However,<br>it is not optimal to allocate and free the memory for every operation. Most of the MPI<br>implementations usually utilize a buffer pool, a common technique which allocates a chunk<br>of memory in advance, and provide a mechanism to request and return the memory to the<br>pool to enable the reuse of the memory, and avoid the costly memory allocation in time-<br>critical operations.</p>
<h1 id='549' style='font-size:20px'>Resource Allocation</h1>
<p id='550' data-category='paragraph' style='font-size:14px'>One major difference between using multiple MPI processes versus a single MPI process<br>with multiple threads is the resources allocated for MPI operations. Resources such as<br>buffer pools, network contexts and endpoints, or CQs are generally created per MPI process.<br>In the process-to-process communication model, with this single producer-single consumer<br>relationship, resource contention is limited. In the case of multiple threads in the same<br>MPI process, these resources have to be protected, as concurrent access to a resource may<br>not be supported, or might create race conditions that could compromise the correctness of<br>the communication or even corrupt the state of the MPI library. At the same time, this<br>protection adds an extra cost to the operation, and the cost often increases with the number<br>of concurrent threads. Moreover, the protection effectively eliminates any opportunity for<br>performing network operation in parallel.</p>
<h1 id='551' style='font-size:22px'>Matching Process</h1>
<p id='552' data-category='paragraph' style='font-size:16px'>The matching engine is another important piece of an MPI implementation for handling<br>incoming messages, as it is responsible for the correct matching of sends and receives.</p>
<p id='553' data-category='paragraph' style='font-size:16px'>For single-threaded applications, the MPI standard offers the guarantee that all messages<br>between a source and destination pair on the same MPI communicator are matched in FIFO<br>order, ensuring that the send order is the same as the matching order. This simplifies the</p>
<footer id='554' style='font-size:18px'>79</footer>
<p id='555' data-category='paragraph' style='font-size:14px'>semantics for the MPI users, as it ensures that, in single-threaded applications, with the<br>same peer, messages are always delivered in each communicator in a deterministic order.</p>
<p id='556' data-category='paragraph' style='font-size:16px'>However, at the network level, the story is different. For performance and routing<br>optimization reasons, networks do not provide any ordering guarantee by default and the<br>messages might be delivered in an arbitrary order. This requires the MPI library to<br>implement a software solution to provide users with the required message ordering guarantee.<br>For multi-threaded usage, the MPI standard only guarantees message ordering within a single<br>thread. Messages sent from different threads are only guaranteed to happen in some serialized<br>order, as MPI communications, even blocking, are not synchronizing.</p>
<p id='557' data-category='paragraph' style='font-size:16px'>The algorithms to provide message ordering may be different for each MPI implementa-<br>tion, but they share a common approach: generate a sequence number for each message and<br>pack it within the message header. For simplicity, this sequence number is generally per peer<br>per communicator. The receiver extracts the sequence number from the incoming header<br>and uses it to ensure messages are processed in the same order they were sent. Any message<br>arriving out of sequence needs to be saved for matching at a later time when that message<br>sequence number is called for. The implementation has to allocate the necessary memory<br>to store the out-of-sequence messages, adding an extra overhead to the operation. The out-<br>of-sequence messages can occur in a single-threaded scenario, where sometimes the network<br>device determines to switch the sending order of messages for optimization reasons but<br>the occurrence is usually very rare, and therefore the overhead is negligible. However, this<br>is not the case for multi-threaded MPI. In the scenario with multiple threads concurrently<br>sending messages on the same communicator to the same destination MPI process, given<br>the nature of their non-deterministic behavior, threads can easily compete and send the<br>messages out of order. With more likelihood of out-of-sequence messages, multi-threaded<br>MPI could suffer significant performance degradation with an increasing number of threads<br>from the out-of-sequence message handling overhead.</p>
<p id='558' data-category='paragraph' style='font-size:14px'>After the MPI implementation successfully validates the sequence number of an incoming<br>message, the message is matched against a queue of the user's posted receives. This code<br>region is a critical section and must be protected with a lock in a multi-thread scenario to<br>prevent concurrent access to the queue. For example, races can occur when threads are</p>
<footer id='559' style='font-size:20px'>80</footer>
<p id='560' data-category='paragraph' style='font-size:14px'>simultaneously posting receives; or when a thread adds a request to the posted receive queue<br>while another thread is in the progress engine trying to match an incoming message with<br>a request on the same queue. The matching lock is mandatory for the correctness of MPI<br>operation in a multi-threaded environment, but it also serves as a huge critical section that<br>prevents the MPI implementation from achieving more thread parallelism.</p>
<p id='561' data-category='paragraph' style='font-size:16px'>The matching process plays a significant role in determining the latency of the two-sided<br>communication, especially in a multi-threaded environments. Currently, there are many<br>efforts to improve the matching process. Network hardware vendors such as Mellanox, Intel,<br>and Cray incorporate the matching process into the hardware itself. The recent network<br>hardware with tag-matching capabilities relieves stress from the software stack, such as<br>MPI, from implementing their own solution. However, this approach moves the burden onto<br>the hardware, which might not have enough software-level information to make the right<br>optimization decision. On the other hand, researchers are studying multiple techniques to<br>speed up the entire matching process, ranging from utilizing the vector instruction for fuzzy<br>matching Schonbein et al. (2018) to the algorithmic approaches such as Flajslik et al. (2016).<br>Nonetheless, there is still no working implementation to utilize multiple threads to perform<br>message matching simultaneously.</p>
<h1 id='562' style='font-size:20px'>Remote Memory Access</h1>
<p id='563' data-category='paragraph' style='font-size:16px'>In addition to two-sided communication, the MPI-3.1 standard provides support for one-<br>sided (RMA) communication. This support allows an MPI implementation to directly<br>expose hardware RDMA, a feature which is present on most high-performance networks (e.g.,<br>Infiniband and Cray Aries). This allows the MPI implementation to offload communication<br>directly to the hardware. In addition, the one-sided model separates communication (data<br>movement) from the synchronization (completion). There is no need for any explicit<br>matching for one-sided communication, removing a potential multi-threaded bottleneck.<br>This makes RMA well suited for multi-threaded applications, but at the same time, it moves<br>the burden of the synchronization to the user, and potentially increases the complexity and<br>readability of the application's code.</p>
<footer id='564' style='font-size:18px'>81</footer>
<p id='565' data-category='paragraph' style='font-size:16px'>With the current MPI standard there is support for three different types of communi-<br>cation operations: put (remote write), get (remote read), and accumulate (remote atomic);<br>and for two classes of synchronization: active-target (fence, post-start-complete-wait), and<br>passive-target (lock, flush). Active-target requires the target MPI process of an RMA<br>operation to participate in the synchronization of the window. It is not well suited for multi-<br>threaded applications, as all synchronization needs to be funneled through a single thread.<br>Passive-target flush, on the other hand, does not require the target of an RMA operation<br>to participate in either the communication or synchronization and allows for concurrent<br>synchronization.</p>
<h1 id='566' style='font-size:22px'>5.3 Design and Implementation</h1>
<p id='567' data-category='paragraph' style='font-size:16px'>This section presents the designs to allow true thread concurrency in the MPI implemen-<br>tation. The goal is to optimize for maximum thread parallelism by giving them proper<br>resources, removing any unnecessary critical sections to create more opportunity for threads<br>to collaborate instead of racing against one another.</p>
<h1 id='568' style='font-size:20px'>5.3.1 Communication Resources Instance</h1>
<p id='569' data-category='paragraph' style='font-size:14px'>There are a variety of critical internal MPI resources that must be protected in a multi-<br>threaded environment, such as the network endpoints, network contexts, and CQs. In<br>existing MPI implementations, a single network context is typically created per MPI process<br>and a single network endpoint per source/ destination pair. The CQ is usually attached to the<br>network context to store completion events. For multi-threaded MPI, access to both network<br>contexts and their CQs may have to be protected, thus creating a potential bottleneck.</p>
<p id='570' data-category='paragraph' style='font-size:16px'>To give multi-threaded MPI a fair chance, more resources have to be allocated for the<br>entire MPI process. I introduce the concept of a Communication Resources Instance (CRI)<br>to encompass resources such as network contexts, network endpoints, and CQs with a per-<br>instance level of protection to perform communication operations. The MPI implementation<br>can allocate multiple CRIs internally for multi-threaded needs.</p>
<footer id='571' style='font-size:18px'>82</footer>
<p id='572' data-category='paragraph' style='font-size:16px'>Currently, there is no interoperability between threading frameworks such as POSIX<br>threads and MPI; therefore, the MPI implementation does not have a standardized way to<br>get the number of threads that will be used for MPI communication from the application.<br>Thus, it is challenging for the implementation to assess the proper number of CRIs to<br>allocate. That being said, an implementation can provide the user with a way to give a hint<br>via environment variable(s), MPI info key(s), or other means (MCA parameters Squyres<br>for Open MPI Gabriel et al. (2004) or the new MPI control variables MPI_T_cvar) to<br>let the implementation know how many threads the application will use for concurrent<br>MPI operations. The implementation can then allocate the CRIs accordingly. In my<br>implementation, MPI allocates a set of CRIs into a resource pool and creates a centralized<br>body to assign the allocated instances to threads.</p>
<p id='573' data-category='paragraph' style='font-size:14px'>Ideally, there should be a one-to-one thread to CRI mapping to completely eliminate<br>the potential for lock contention. However, in some cases, there might be a limit to the<br>resources available for creating CRIs. Some network devices, such as Cray Aries, might have<br>a hardware limitation on the number of network contexts the user can create, SO the design<br>must also accommodate cases where the number of CRIs is less than the number of threads.</p>
<p id='574' data-category='paragraph' style='font-size:14px'>Giving more resources to the threads might not be sufficient to increase communication<br>performance for two-sided communication, as the MPI implementation still serializes the calls<br>to both the send operation and progress engine to prevent any potential race conditions. In<br>order to benefit from more allocated resources, both the send and receive paths have to be<br>redesigned to allow for more parallelism while maintaining thread safety and continuing to<br>ensure the expected matching semantic.</p>
<p id='575' data-category='paragraph' style='font-size:20px'>5.3.2 Try-lock Semantics</p>
<p id='576' data-category='paragraph' style='font-size:14px'>Using locks to protect critical resources is one of the popular approaches to ensure thread<br>safety. These locks also act as a funnel when multiple threads are going through the same<br>code path as lock contention will cause threads to block. We can mitigate the funneling<br>effect by using try-lock semantics, which is a non-blocking version of lock, where it will<br>return immediately after it fails to acquire the lock.</p>
<footer id='577' style='font-size:18px'>83</footer>
<p id='578' data-category='paragraph' style='font-size:14px'>Try-lock semantics provide more opportunities for parallelism. When the lock is already<br>taken, we can be certain that a thread is progressing that particular code path, and, therefore,<br>the current thread can move on and try to pick up another code path to execute or become<br>a helper thread and complete other menial work.</p>
<br><p id='579' data-category='paragraph' style='font-size:14px'>The following subsections describe how to leverage the try-lock semantics with the<br>communication resources instances (I will further refer to them as CRIs or "instance" in the<br>following sections), to alleviate resource contention from MPI's internal message extraction<br>process.</p>
<h1 id='580' style='font-size:22px'>5.3.3 Concurrent Sends</h1>
<p id='581' data-category='paragraph' style='font-size:14px'>For the MPI implementation to perform a send operation, it needs access to a network<br>endpoint. In the multi-threaded case, the implementation usually protects the network<br>context with a lock. In this new design, the network context is associated with a CRI along<br>with other resources. The protection is changed from per-endpoint level to per-instance level,<br>allowing the threads to perform send operations simultaneously on different instances. To<br>assign a CRI to a thread, I propose two strategies: round-robin and dedicated (Algorithm<br>3).</p>
<h1 id='582' style='font-size:20px'>Round-Robin Assignment</h1>
<p id='583' data-category='paragraph' style='font-size:14px'>In this strategy, every time a thread needs to communicate it first acquires a CRI. The MPI<br>implementation assigns an instance for single use on a first-come, first-served manner. Once<br>the last available instance is assigned, the implementation will recycle the instances and then<br>give out the first instance again. This approach reduces the possibility of lock contention by<br>assigning a different instance for every call. It also improves load balancing by giving a fair<br>share of work among the allocated instances.</p>
<h1 id='584' style='font-size:22px'>Dedicated Assignment</h1>
<p id='585' data-category='paragraph' style='font-size:18px'>To permanently assign a CRI to a thread, Message Passing Interface (MPI) can utilize<br>Thread-Local Storage (TLS), provided either by the threading library (e.g., POSIX threads)</p>
<footer id='586' style='font-size:16px'>84</footer>
<p id='587' data-category='paragraph' style='font-size:22px'>Algorithm 3 Utilizing multiple CRIs to allow concurrent sends.</p>
<br><p id='588' data-category='paragraph' style='font-size:16px'>1: function INIT</p>
<br><p id='589' data-category='list' style='font-size:18px'>2: for i ← 1, NumInstances do<br>3: instance[i] ←CREATE- INSTANCE()</p>
<p id='590' data-category='paragraph' style='font-size:20px'>4: function SEND(msg)</p>
<br><p id='591' data-category='list' style='font-size:20px'>5: k ← GET-INSTANCE-ID()<br>6: LOCK(instance[k] → lock)<br>7: NETWORKSEND(instonce[k], msg)<br>8: UNLOCK(instancek] → lock)</p>
<p id='592' data-category='paragraph' style='font-size:14px'>9: function GET-INSTANCE-ID-ROUND-ROBIN</p>
<br><p id='593' data-category='list' style='font-size:14px'>10: static current_id ← 0<br>11: ret = current_id<br>12: current_id ← current_id + 1<br>13: return (ret mod numInstances )</p>
<h1 id='594' style='font-size:14px'>14: function GET-INSTANCE-ID-DEDICATED</h1>
<br><table id='595' style='font-size:16px'><tr><td>15:</td><td>static thread _local my_id ← undefined</td></tr><tr><td>16:</td><td>if my_id is defined then</td></tr><tr><td>17:</td><td>return my_id</td></tr><tr><td>18:</td><td>else</td></tr><tr><td>19:</td><td>my_id ← GET-INSTANCE-ID() -ROUND-ROBIN</td></tr><tr><td>20:</td><td>return my_id</td></tr></table>
<p id='596' data-category='paragraph' style='font-size:18px'>or the programming language (e.g., C11, C++11). This approach can only be implemented<br>when the system or the compiler supports TLS, a pretty standard feature nowadays. My<br>implementation uses the native compiler support either from C11 or GCC. When checking for<br>a CRI to use, the implementation can check if instance information is stored in TLS. If not,<br>it can assign an instance with a round-robin assignment and save the instance information<br>in the TLS. With a dedicated assignment strategy, there is no possibility of lock contention<br>on the instance as long as the number of threads is lower or equal to the number of instances<br>allocated. If not, some communicating threads might share the same instance and might<br>even introduce some lock contention if they simultaneously communicate.</p>
<footer id='597' style='font-size:20px'>85</footer>